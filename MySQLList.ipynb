{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6b42e4-fd4e-454e-a694-734a24780d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import MySQLdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "import os\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4076de3-f64d-4f72-9731-4efcd5f477b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_edge_index_iterative(source_nodes, edge_index, max_depth):\n",
    "    \"\"\"\n",
    "    Optimized function to compute the subgraph around the source nodes up to a given depth.\n",
    "    Uses an iterative approach instead of recursion.\n",
    "    \"\"\"\n",
    "    visited_nodes = set(source_nodes)\n",
    "    current_frontier = np.array(source_nodes)\n",
    "    \n",
    "    subgraph_edges = []\n",
    "\n",
    "    for _ in range(max_depth):\n",
    "        # Find edges where the target node is in the current frontier\n",
    "        target_mask = np.isin(edge_index[1], current_frontier)\n",
    "        subgraph_edge_index = edge_index[:, target_mask]\n",
    "        subgraph_edges.append(subgraph_edge_index)\n",
    "\n",
    "        # Update the current frontier with the source nodes of these edges\n",
    "        current_frontier = np.setdiff1d(subgraph_edge_index[0], list(visited_nodes))\n",
    "        visited_nodes.update(current_frontier)\n",
    "        \n",
    "        if len(current_frontier) == 0:\n",
    "            break\n",
    "\n",
    "    # Combine edges from all hops\n",
    "    return np.concatenate(subgraph_edges, axis=1) if subgraph_edges else np.empty((2, 0), dtype=edge_index.dtype)\n",
    "\n",
    "\n",
    "def get_subgraph_from_in_mem_graph_optimized(X, y, i, edge_index, hops):\n",
    "    \"\"\"\n",
    "    Optimized version of subgraph extraction.\n",
    "    \"\"\"\n",
    "    subgraph_edge_index = recurse_edge_index_iterative([i], edge_index, hops)\n",
    "    unique_node_ids, remapping = np.unique(subgraph_edge_index, return_inverse=True)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = X.iloc[unique_node_ids, :].values\n",
    "    labels = y.iloc[unique_node_ids, :].values.squeeze()\n",
    "\n",
    "    # Remap edge indices\n",
    "    remapped_edge_index = remapping.reshape(2, -1)\n",
    "    return remapped_edge_index, features, labels, unique_node_ids\n",
    "    \n",
    "def connect_to_mysql(database=None):\n",
    "    \"\"\"Connect to MySQL database using mysqlclient\"\"\"\n",
    "    if database is None:\n",
    "        return MySQLdb.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        passwd='password',\n",
    "        local_infile=True# Enable LOAD DATA LOCAL INFILE\n",
    "    )\n",
    "    return MySQLdb.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        passwd='password',\n",
    "        db=database,\n",
    "        local_infile=True# Enable LOAD DATA LOCAL INFILE\n",
    "    )\n",
    "\n",
    "def create_database_with_conn(conn, db_name):\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "        conn.commit()\n",
    "    return connect_to_mysql(database=db_name)\n",
    "\n",
    "def create_database(node_file_name):\n",
    "    conn = connect_to_mysql()\n",
    "    new_db_name = node_file_name.split(\".\")[0]\n",
    "    create_database_with_conn(conn, new_db_name)\n",
    "    conn.close()\n",
    "    conn = connect_to_mysql(new_db_name)\n",
    "    return conn, new_db_name\n",
    "\n",
    "def create_index(conn, table_name, column_name):\n",
    "    \"\"\"Create an index on a specific column.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            index_name = f\"{table_name}_{column_name}_idx\"\n",
    "            cursor.execute(f\"CREATE INDEX {index_name} ON {table_name} ({column_name});\")\n",
    "            conn.commit()\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating index: {e}\")\n",
    "        raise\n",
    "\n",
    "def create(conn, node_file_name, edge_file_name, X_and_y):\n",
    "    # For each column, we'll store arrays as TEXT with comma-separated values\n",
    "    column_types = [\"id INTEGER PRIMARY KEY\"] ## AUTO INCREMENT\n",
    "    for col in X_and_y.columns:\n",
    "        column_types.append(f\"{col} JSON\")\n",
    "    \n",
    "    node_schema = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS nodes (\n",
    "        {\",\".join(column_types)}\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(node_schema)\n",
    "        \n",
    "        # Create edges table\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS edges (\n",
    "            source_id INTEGER,\n",
    "            target_id INTEGER\n",
    "        )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "    \n",
    "    csv_file_path = f\"syn_data/{node_file_name}\"\n",
    "    \n",
    "    # For the nodes table, we need to preprocess the CSV to convert arrays to strings\n",
    "    with conn.cursor() as cursor:\n",
    "        # MySQLdb requires slightly different syntax for LOAD DATA LOCAL INFILE\n",
    "        load_data_sql = f\"\"\"\n",
    "        LOAD DATA LOCAL INFILE %s\n",
    "        INTO TABLE nodes\n",
    "        FIELDS TERMINATED BY ';'\n",
    "        LINES TERMINATED BY '\\\\n'\n",
    "        IGNORE 1 LINES\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(load_data_sql, (csv_file_path,))#\n",
    "        create_index(conn, \"nodes\", \"id\")\n",
    "        \n",
    "        # Load edges\n",
    "        cursor.execute(\n",
    "            \"LOAD DATA LOCAL INFILE %s INTO TABLE edges FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\\\n' IGNORE 1 LINES\",\n",
    "            (f\"syn_data/{edge_file_name}\",)\n",
    "        )\n",
    "        create_index(conn, \"edges\", \"target_id\")\n",
    "        conn.commit()\n",
    "    \n",
    "    creation_time = time.time() - start\n",
    "    return creation_time\n",
    "\n",
    "def read(conn, hops, X_and_y, X, y, edge_index, random_sample_size = 1_000):\n",
    "    np.random.seed(42)\n",
    "    seed_node_ids = np.random.choice(np.arange(X_and_y.shape[0]), size = random_sample_size, replace = False)\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SET SESSION group_concat_max_len = 86777215;\")\n",
    "        cursor.execute(\"SET GLOBAL max_allowed_packet = 8073741824;\")  # 1GB\n",
    "\n",
    "        complete_time = 0\n",
    "        complete_test_time = 0\n",
    "        \n",
    "        for seed_node_id in tqdm(seed_node_ids):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                cursor.execute(f\"\"\"\n",
    "        WITH RECURSIVE NestedTargets AS (\n",
    "            SELECT 0 AS depth, source_id, target_id\n",
    "            FROM edges\n",
    "            WHERE target_id = {seed_node_id}\n",
    "            \n",
    "            UNION ALL\n",
    "            \n",
    "            SELECT nt.depth + 1, e.source_id, e.target_id\n",
    "            FROM edges e\n",
    "            INNER JOIN NestedTargets nt ON e.target_id = nt.source_id\n",
    "            WHERE nt.depth < {hops - 1}\n",
    "        ),\n",
    "        \n",
    "        node_ids AS (\n",
    "            SELECT DISTINCT id FROM (\n",
    "                SELECT source_id AS id FROM NestedTargets\n",
    "                UNION\n",
    "                SELECT target_id AS id FROM NestedTargets\n",
    "            ) AS combined_ids\n",
    "        ),\n",
    "        \n",
    "        node_data AS (\n",
    "            SELECT \n",
    "                id, \n",
    "                {\", \".join(X_and_y.columns)}\n",
    "            FROM nodes\n",
    "            WHERE id IN (SELECT id FROM node_ids)\n",
    "        )\n",
    "        \n",
    "        SELECT\n",
    "        (SELECT JSON_ARRAYAGG(X) FROM node_data) AS node_table,\n",
    "        (SELECT JSON_ARRAYAGG(y) FROM node_data) AS label_table,\n",
    "        (SELECT JSON_ARRAYAGG(JSON_ARRAY(source_id, target_id))\n",
    "         FROM (SELECT DISTINCT source_id, target_id FROM NestedTargets) AS edges) AS edge_table,\n",
    "         (SELECT JSON_ARRAYAGG(id) FROM node_data) AS node_ids;\n",
    "    \"\"\")\n",
    "                results = cursor.fetchall()[0]\n",
    "                if None in results:\n",
    "                    continue\n",
    "                labels = np.array(json.loads(results[1]))\n",
    "                subgraph_node_features = np.array(json.loads(results[0]))\n",
    "                if results[0] is None:\n",
    "                    continue\n",
    "                \n",
    "                subgraph_edges = np.array(json.loads(results[2])).transpose()\n",
    "                \n",
    "                node_ids = np.array(json.loads(results[-1]))\n",
    "                ##TODO change in other files\n",
    "                id_sort_idx = np.argsort(node_ids)\n",
    "                node_ids = node_ids[id_sort_idx]\n",
    "                features = subgraph_node_features[id_sort_idx]\n",
    "                labels = labels[id_sort_idx]\n",
    "                cols_source = np.searchsorted(node_ids, subgraph_edges[0])\n",
    "                cols_target = np.searchsorted(node_ids, subgraph_edges[1])\n",
    "                remapped_edge_index = np.concatenate([np.expand_dims(cols_source, axis = 0), np.expand_dims(cols_target, axis = 0)], axis = 0)\n",
    "                overall_run_time = time.time() - start \n",
    "                \n",
    "                complete_time += overall_run_time\n",
    "                # Testing\n",
    "                test_time = time.time()\n",
    "                remapped_edge_index_test, features_test, labels_test, unique_node_ids = get_subgraph_from_in_mem_graph_optimized(X, y, seed_node_id, edge_index, hops)                    \n",
    "                complete_test_time += time.time() - test_time\n",
    "\n",
    "                assert (sort_edge_index(torch.from_numpy(remapped_edge_index_test)) == sort_edge_index(torch.from_numpy(remapped_edge_index))).sum() / (remapped_edge_index_test.shape[-1] * remapped_edge_index_test.shape[0]), \"Edges doesnt match\"\n",
    "                del remapped_edge_index_test\n",
    "                del remapped_edge_index\n",
    "                gc.collect()\n",
    "                assert np.allclose(node_ids, unique_node_ids), \"Node ids does not match\"\n",
    "                del node_ids\n",
    "                del unique_node_ids\n",
    "                gc.collect()\n",
    "                assert np.allclose(features, features_test), \"features does not match\"\n",
    "                del features\n",
    "                del features_test\n",
    "                gc.collect()\n",
    "                # assert np.allclose(labels_test, labels), \"Labels does not match\"\n",
    "                # print(f\"Fetched {remapped_edge_index.shape} edges, {labels.shape} labels, {features.shape} features in ({overall_run_time} s)\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                print(f\"Error reading data: {e}\")\n",
    "                raise   \n",
    "    return (complete_time, complete_test_time)\n",
    "\n",
    "def update_nodes(conn, X_and_y, X, y, random_sample_size = 1000):\n",
    "    with conn.cursor() as cursor:\n",
    "        np.random.seed(42)\n",
    "        node_ids = np.random.choice(np.arange(X_and_y.shape[0]), size = random_sample_size, replace = False).tolist()\n",
    "        start = time.time()\n",
    "        for node_id in tqdm(node_ids):\n",
    "            np.random.seed(42)\n",
    "            features = np.random.rand(X.shape[-1]).tolist()  # Random values between 0 and 1\n",
    "            labels = np.random.randint(0, 2, size=y.shape[-1]).tolist() \n",
    "            sql_query = f\"\"\"\n",
    "            UPDATE nodes \n",
    "            SET X = '%s', y = '%s'\n",
    "            WHERE id = %s;\n",
    "            \"\"\"\n",
    "            values = (features, labels, node_id)\n",
    "            cursor.execute(sql_query % values)\n",
    "        return time.time() - start\n",
    "\n",
    "def update_edges(conn, edge_index, X_and_y, random_sample_size = 1000):\n",
    "    with conn.cursor() as cursor:\n",
    "        np.random.seed(42)\n",
    "        edge_ids = np.random.choice(np.arange(edge_index.shape[-1]), size = random_sample_size, replace = False).tolist()\n",
    "        selected_edges = edge_index[:, edge_ids].transpose(-1, 0 )\n",
    "        start = time.time()\n",
    "        for selected_edge in tqdm(selected_edges):\n",
    "            source_id, target_id = selected_edge\n",
    "            np.random.seed(42)\n",
    "            new_target_id = int(np.random.randint(0, X_and_y.shape[0]))\n",
    "            sql_query = \"UPDATE edges SET target_id = %s WHERE source_id = %s AND target_id = %s;\"\n",
    "            values = (new_target_id, int(source_id), int(target_id))\n",
    "            cursor.execute(sql_query, values)\n",
    "        return time.time() - start\n",
    "\n",
    "def delete_database(conn, db_name):\n",
    "    \"\"\"Delete the specified database including all its schemas, tables, and indexes.\"\"\"\n",
    "    try:\n",
    "        conn.autocommit = True\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(f\"DROP DATABASE IF EXISTS {db_name};\")\n",
    "            print(f\"Database '{db_name}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.autocommit = False\n",
    "\n",
    "\n",
    "def delete(conn, new_db_name):\n",
    "    start = time.time()\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "    conn = connect_to_mysql()\n",
    "    delete_database(conn, new_db_name)\n",
    "    conn.close()\n",
    "    return time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17dd90-93ce-42de-b298-7ed66c29b29e",
   "metadata": {},
   "source": [
    "## PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b909a3-3238-481d-87c5-8204871717de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_ppi(output_df):\n",
    "    edge_file_name = \"ppi_edge_index.csv\"\n",
    "    node_file_name = \"ppi.csv\"\n",
    "    X = pd.read_csv(f\"data/ppi_x.csv\")\n",
    "    y = pd.read_csv(f\"data/ppi_y.csv\")\n",
    "    edges = pd.read_csv(\"data/\" + edge_file_name)\n",
    "    edges.columns = [\"source_id\", \"target_id\"]\n",
    "    \n",
    "    X_and_y = pd.DataFrame()\n",
    "    X_and_y[\"y\"] = y.apply(lambda row: [int(row[column]) for column in y.columns], axis=1)\n",
    "    X_and_y[\"X\"] = X.apply(lambda row: [row[column] for column in X.columns], axis=1)\n",
    "    \n",
    "    node_file_name = \"X_y_ppi_mysql_list.csv\" ## Can use the same structure for all database when using column wise import\n",
    "    X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \";\", index = True)\n",
    "    edges.to_csv(f\"syn_data/{edge_file_name}\", sep = \",\", index = False)\n",
    "    edge_index = edges.values.transpose(-1, 0)\n",
    "    conn, db_name = create_database(node_file_name)\n",
    "    create_time = create(conn, node_file_name, edge_file_name, X_and_y)\n",
    "    read_times =dict()\n",
    "    read_times_mem =dict()\n",
    "    for hops in range(1,4):\n",
    "        read_time, read_time_mem = read(conn, hops, X_and_y, X, y, edge_index, random_sample_size = 1_000)\n",
    "        read_times_mem[hops] = read_time_mem\n",
    "        read_times[hops] = read_time\n",
    "    update_node_time = update_nodes(conn, X_and_y, X, y, 1_000)\n",
    "    update_edge_time = update_edges(conn, edge_index, X_and_y, 1_000)\n",
    "    delete_time= delete(conn,db_name )\n",
    "    \n",
    "    new_row_dict = {\"name\": \"PPI\", \"create\": create_time, \"update_nodes\": update_node_time, \"update_edges\": update_edge_time, \"delete\": delete_time}\n",
    "    for hops in read_times:\n",
    "        new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "        new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "    new_row = pd.DataFrame([new_row_dict])\n",
    "    output_df = pd.concat((output_df, new_row), ignore_index=True)\n",
    "    return output_df\n",
    "\t\n",
    "def eval_synth(output_df):\n",
    "    for num_nodes in tqdm([1_000, 10_000, 100_000, 1_000_000]):\n",
    "        for num_edges in tqdm([\"5_edges\", \"10_edges\", \"20_edges\", \"scale_free\"]):\n",
    "            if num_nodes == 1_000_000 and (num_edges == \"10_edges\" or num_edges == \"20_edges\"): continue\n",
    "            feature_file_name = f\"X_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            label_file_name = f\"y_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            edge_file_name = f\"edge_index_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            assert os.path.exists(f\"syn_data/{feature_file_name}\"), \"Feature file does not exist\"\n",
    "            assert os.path.exists(f\"syn_data/{label_file_name}\"), \"Label file does not exist\"\n",
    "            assert os.path.exists(f\"syn_data/{edge_file_name}\"), \"Edge file does not exist\"\n",
    "            \n",
    "            X = pd.read_csv(f\"syn_data/{feature_file_name}\")\n",
    "            y = pd.read_csv(f\"syn_data/{label_file_name}\")\n",
    "            y.columns = [f\"y_{col}\" for col in y.columns]\n",
    "            y = y.astype(np.int8)\n",
    "            edges = pd.read_csv(f\"syn_data/{edge_file_name}\")\n",
    "            edges.columns = [\"source_id\", \"target_id\"]\n",
    "            edge_index = edges.values.transpose(-1, 0)\n",
    "            node_file_name = f\"X_and_y_{str(num_nodes)}_nodes_{num_edges}_mysql_list.csv\"\n",
    "            if True or not os.path.exists(f\"syn_data/{node_file_name}\"):\n",
    "                X_and_y = pd.DataFrame()\n",
    "                X_and_y[\"y\"] = y.apply(lambda row: [int(row[column]) for column in y.columns], axis=1)\n",
    "                X_and_y[\"X\"] = X.apply(lambda row: [row[column] for column in X.columns], axis=1)\n",
    "                X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \";\", index = True)\n",
    "            else:\n",
    "                X_and_y = pd.read_csv(f\"syn_data/{node_file_name}\", sep = \";\", index_col = 0)\n",
    "            conn, db_name = create_database(node_file_name)\n",
    "            create_time = create(conn, node_file_name, edge_file_name, X_and_y)\n",
    "            read_times = dict()\n",
    "            read_times_mem = dict()\n",
    "            for hops in tqdm(range(1, 4)):\n",
    "                read_time, read_time_mem = read(conn, hops, X_and_y, X, y, edge_index, random_sample_size = 1_000)\n",
    "                read_times[hops] = read_time\n",
    "                read_times_mem[hops] = read_time_mem\n",
    "            update_time_nodes = update_nodes(conn, X_and_y,  X, y, 1_000)\n",
    "            update_time_edges = update_edges(conn, edge_index, X_and_y, 1_000)\n",
    "            delete_time = delete(conn, db_name)\n",
    "            new_row_dict = {\"name\": f\"{str(num_nodes)}_nodes_{num_edges}\", \"create\": create_time, \"update_nodes\": update_time_nodes, \"update_edges\": update_time_edges, \"delete\": delete_time}\n",
    "            for hops in read_times:\n",
    "                new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "                new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "            new_row = pd.DataFrame([new_row_dict])\n",
    "            output_df = pd.concat((output_df, new_row), ignore_index=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded1c959-ebbe-45c5-9fad-cdea5f9fb4ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(off_set, num_iterations \u001b[38;5;241m+\u001b[39m off_set):\n\u001b[1;32m      4\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_edges\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppi\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m eval_synth(output_df)\n\u001b[1;32m      7\u001b[0m     output_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmysql_list_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36meval_ppi\u001b[0;34m(output_df)\u001b[0m\n\u001b[1;32m      9\u001b[0m X_and_y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     10\u001b[0m X_and_y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [\u001b[38;5;28mint\u001b[39m(row[column]) \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m y\u001b[38;5;241m.\u001b[39mcolumns], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m X_and_y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m node_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_y_ppi_mysql_list.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m## Can use the same structure for all database when using column wise import\u001b[39;00m\n\u001b[1;32m     14\u001b[0m X_and_y\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyn_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:10347\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10333\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10335\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10337\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10345\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10346\u001b[0m )\n\u001b[0;32m> 10347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36meval_ppi.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      9\u001b[0m X_and_y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     10\u001b[0m X_and_y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [\u001b[38;5;28mint\u001b[39m(row[column]) \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m y\u001b[38;5;241m.\u001b[39mcolumns], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m X_and_y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [row[column] \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m node_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_y_ppi_mysql_list.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m## Can use the same structure for all database when using column wise import\u001b[39;00m\n\u001b[1;32m     14\u001b[0m X_and_y\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyn_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m X_and_y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     10\u001b[0m X_and_y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [\u001b[38;5;28mint\u001b[39m(row[column]) \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m y\u001b[38;5;241m.\u001b[39mcolumns], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m X_and_y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [row[column] \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m node_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_y_ppi_mysql_list.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m## Can use the same structure for all database when using column wise import\u001b[39;00m\n\u001b[1;32m     14\u001b[0m X_and_y\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyn_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "off_set = 0\n",
    "for i in range(off_set, num_iterations + off_set):\n",
    "    print(f\"Iteration {i}\")\n",
    "    output_df = pd.DataFrame(columns = [\"name\", \"create\", \"update_nodes\", \"update_edges\", \"delete\"])\n",
    "    output_df = eval_ppi(output_df)\n",
    "    output_df = eval_synth(output_df)\n",
    "    output_df.to_csv(f\"mysql_list_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c840c6e-94e9-4992-9aa7-8b8d829fbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect_to_mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e86ba6-b96d-4fc8-a341-28719b6c202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('information_schema',), ('mysql',), ('performance_schema',), ('sys',))\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"SHOW DATABASES;\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73cad790-de6b-416b-afd7-a7c7bab36b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"DROP DATABASE X_and_y_1000000_nodes_scale_free_mysql_list;\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e43e57-dd78-4df7-9a2e-4847ddbdc7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
