{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6b42e4-fd4e-454e-a694-734a24780d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4076de3-f64d-4f72-9731-4efcd5f477b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_execution_time(explain_output):\n",
    "    \"\"\"\n",
    "    Extracts the total execution time from the given query plan.\n",
    "\n",
    "    The function parses the query plan to find the execution time, which is typically\n",
    "    represented in the format 'Execution Time: X ms'. It returns the execution time\n",
    "    in seconds.\n",
    "\n",
    "    Parameters:\n",
    "    query_plan (list): A list of strings representing the lines of the query plan output.\n",
    "\n",
    "    Returns:\n",
    "    float: The total execution time in ms. If the execution time cannot be found,\n",
    "           returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    execution_time = 0.0\n",
    "    pattern = re.compile(r\"Execution Time: (\\d+\\.\\d+) ms\")\n",
    "    for row in explain_output:\n",
    "        match = pattern.search(row[0])\n",
    "        if match:\n",
    "            execution_time += float(match.group(1))\n",
    "    return execution_time\n",
    "    \n",
    "def connect_to_postgres(dbname = \"postgres\"):\n",
    "    \"\"\"Connect to the PostgreSQL database server.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,  # Connect to default db to create new db\n",
    "            user='postgres',\n",
    "            password='password',\n",
    "            host='localhost'\n",
    "        )\n",
    "        print(\"Connection successful.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_database(conn, new_db_name):\n",
    "    \"\"\"Create a new database.\"\"\"\n",
    "    try:\n",
    "        conn.autocommit = True\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql.SQL(\"CREATE DATABASE {};\").format(sql.Identifier(new_db_name)))\n",
    "            print(f\"Database '{new_db_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.autocommit = False\n",
    "\n",
    "def create_schema(conn, schema_sql):\n",
    "    \"\"\"Create the database schema.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(schema_sql)\n",
    "            conn.commit()\n",
    "            print(\"Schema created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating schema: {e}\")\n",
    "        raise\n",
    "\n",
    "def upload_csv_to_table(conn, csv_file_path, table_name):\n",
    "    \"\"\"Upload a CSV file to a table using the COPY command.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            with open(csv_file_path, 'r') as f:\n",
    "                cursor.copy_expert(\n",
    "                    sql.SQL(\"\"\"\n",
    "                        COPY {} FROM STDIN WITH (FORMAT CSV, HEADER TRUE, DELIMITER ',');\n",
    "                    \"\"\").format(sql.Identifier(table_name)), f\n",
    "                )\n",
    "            conn.commit()\n",
    "            print(f\"Data from '{csv_file_path}' uploaded to table '{table_name}' successfully using COPY.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error uploading CSV data using COPY: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_index(conn, table_name, column_name):\n",
    "    \"\"\"Create an index on a specific column.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            index_name = f\"{table_name}_{column_name}_idx\"\n",
    "            cursor.execute(sql.SQL(\"CREATE INDEX {} ON {} ({});\").format(\n",
    "                sql.Identifier(index_name),\n",
    "                sql.Identifier(table_name),\n",
    "                sql.Identifier(column_name)\n",
    "            ))\n",
    "            conn.commit()\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating index: {e}\")\n",
    "        raise\n",
    "\n",
    "def read_data(conn, query):\n",
    "    \"\"\"Read data from the database.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            print(\"Data read successfully.\")\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def update_data(conn, query, params):\n",
    "    \"\"\"Update data in the database.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, params)\n",
    "            conn.commit()\n",
    "            print(\"Data updated successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error updating data: {e}\")\n",
    "        raise\n",
    "\n",
    "def delete_database(conn, db_name):\n",
    "    \"\"\"Delete the specified database including all its schemas, tables, and indexes.\"\"\"\n",
    "    try:\n",
    "        conn.autocommit = True\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql.SQL(\"DROP DATABASE IF EXISTS {};\").format(sql.Identifier(db_name)))\n",
    "            print(f\"Database '{db_name}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.autocommit = False\n",
    "\n",
    "def create_edges_table(conn):\n",
    "    \"\"\"Create an edges table with foreign key constraints to the main table.\"\"\"\n",
    "    edges_schema = \"\"\"\n",
    "    CREATE TABLE edges (\n",
    "        source_id INTEGER NOT NULL,\n",
    "        target_id INTEGER NOT NULL,\n",
    "        FOREIGN KEY (source_id) REFERENCES nodes(id) ON DELETE CASCADE,\n",
    "        FOREIGN KEY (target_id) REFERENCES nodes(id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(edges_schema)\n",
    "            conn.commit()\n",
    "            print(\"Edges table created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating edges table: {e}\")\n",
    "        raise\n",
    "\n",
    "def upload_edges_csv_to_table(conn, csv_file_path):\n",
    "    \"\"\"Upload a CSV file to the edges table using the COPY command.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            with open(csv_file_path, 'r') as f:\n",
    "                cursor.copy_expert(\n",
    "                    sql.SQL(\"\"\"\n",
    "                        COPY edges (source_id, target_id) FROM STDIN WITH (FORMAT CSV, HEADER TRUE, DELIMITER ',');\n",
    "                    \"\"\"), f\n",
    "                )\n",
    "            conn.commit()\n",
    "            print(f\"Edges data from '{csv_file_path}' uploaded successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error uploading edges CSV data: {e}\")\n",
    "        raise\n",
    "\n",
    "def recurse_edge_index_iterative(source_nodes, edge_index, max_depth):\n",
    "    \"\"\"\n",
    "    Optimized function to compute the subgraph around the source nodes up to a given depth.\n",
    "    Uses an iterative approach instead of recursion.\n",
    "    \"\"\"\n",
    "    visited_nodes = set(source_nodes)\n",
    "    current_frontier = np.array(source_nodes)\n",
    "    \n",
    "    subgraph_edges = []\n",
    "\n",
    "    for _ in range(max_depth):\n",
    "        # Find edges where the target node is in the current frontier\n",
    "        target_mask = np.isin(edge_index[1], current_frontier)\n",
    "        subgraph_edge_index = edge_index[:, target_mask]\n",
    "        subgraph_edges.append(subgraph_edge_index)\n",
    "\n",
    "        # Update the current frontier with the source nodes of these edges\n",
    "        current_frontier = np.setdiff1d(subgraph_edge_index[0], list(visited_nodes))\n",
    "        visited_nodes.update(current_frontier)\n",
    "        \n",
    "        if len(current_frontier) == 0:\n",
    "            break\n",
    "\n",
    "    # Combine edges from all hops\n",
    "    return np.concatenate(subgraph_edges, axis=1) if subgraph_edges else np.empty((2, 0), dtype=edge_index.dtype)\n",
    "\n",
    "\n",
    "def get_subgraph_from_in_mem_graph_optimized(X, y, i, edge_index, hops):\n",
    "    \"\"\"\n",
    "    Optimized version of subgraph extraction.\n",
    "    \"\"\"\n",
    "    subgraph_edge_index = recurse_edge_index_iterative([i], edge_index, hops)\n",
    "    unique_node_ids, remapping = np.unique(subgraph_edge_index, return_inverse=True)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = X.iloc[unique_node_ids, :].values\n",
    "    labels = y.iloc[unique_node_ids, :].values.squeeze()\n",
    "\n",
    "    # Remap edge indices\n",
    "    remapped_edge_index = remapping.reshape(2, -1)\n",
    "    return remapped_edge_index, features, labels, unique_node_ids\n",
    "\n",
    "def create_db(node_file_name):\n",
    "    # Create a new database\n",
    "    conn = connect_to_postgres(dbname = \"postgres\")\n",
    "    new_db_name = node_file_name.split(\".\")[0]\n",
    "    create_database(conn, new_db_name)\n",
    "    conn.close()\n",
    "    conn = connect_to_postgres(new_db_name)\n",
    "    return conn, new_db_name\n",
    "\n",
    "def create(conn, node_file_name, edge_file_name, X_and_y):\n",
    "    column_types = [\"id SERIAL PRIMARY KEY\"]\n",
    "    for col in X_and_y.columns:\n",
    "        if \"y\" in col:\n",
    "            column_types.append(f\"{col} INTEGER\")\n",
    "            continue\n",
    "        column_types.append(f\"{col} REAL\")\n",
    "        \n",
    "    node_schema = f\"\"\"\n",
    "    CREATE TABLE nodes (\n",
    "        {\",\".join(column_types)}\n",
    "    );\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    create_schema(conn, node_schema)\n",
    "    create_edges_table(conn)\n",
    "    \n",
    "    csv_file_path = f\"syn_data/{node_file_name}\"  # Replace with your CSV file path\n",
    "    upload_csv_to_table(conn, csv_file_path, \"nodes\")\n",
    "    create_index(conn, \"nodes\", \"id\")\n",
    "    upload_edges_csv_to_table(conn, f\"syn_data/{edge_file_name}\")\n",
    "    create_index(conn, \"edges\", \"target_id\")\n",
    "    creation_time = time.time() - start\n",
    "    return creation_time\n",
    "\n",
    "def read(conn, hops, X_and_y, X, y, edge_index, random_sample_size = 1_000):\n",
    "    np.random.seed(42)\n",
    "    seed_node_ids = np.random.choice(np.arange(X_and_y.shape[0]), size = random_sample_size, replace = False)\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        complete_time = 0\n",
    "        complete_test_time = 0\n",
    "        feature_columns = list(filter(lambda col: \"f\" in col, X_and_y.columns))\n",
    "        label_columns = list(filter(lambda col: \"y\" in col, X_and_y.columns))\n",
    "        for seed_node_id in tqdm(seed_node_ids):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                cursor.execute(f\"\"\"\n",
    "        WITH RECURSIVE NestedTargets AS (\n",
    "            SELECT 0 AS depth, source_id, target_id\n",
    "            FROM edges\n",
    "            WHERE target_id = {seed_node_id}\n",
    "            \n",
    "            UNION ALL\n",
    "            \n",
    "            SELECT nt.depth + 1, e.source_id, e.target_id\n",
    "            FROM edges e\n",
    "            JOIN NestedTargets nt ON e.target_id = nt.source_id\n",
    "            WHERE nt.depth < {hops - 1}\n",
    "        ),\n",
    "        \n",
    "        node_ids AS (\n",
    "            SELECT DISTINCT id FROM (\n",
    "                SELECT source_id AS id FROM NestedTargets\n",
    "                UNION\n",
    "                SELECT target_id AS id FROM NestedTargets\n",
    "            ) AS combined_ids\n",
    "        ),\n",
    "        \n",
    "        node_data AS (\n",
    "            SELECT \n",
    "                id, \n",
    "                {\", \".join(X_and_y.columns)}\n",
    "            FROM nodes\n",
    "            WHERE id IN (SELECT id FROM node_ids)\n",
    "            ORDER BY id\n",
    "        )\n",
    "        \n",
    "        SELECT\n",
    "            (SELECT array_agg(array[{\", \".join(feature_columns)}]) FROM node_data) AS node_table,\n",
    "            (SELECT array_agg(array[{\", \".join(label_columns)}]) FROM node_data) AS label_table,\n",
    "            (SELECT array_agg(array[source_id, target_id])\n",
    "             FROM (SELECT DISTINCT source_id, target_id FROM NestedTargets) AS edges) AS edge_table,\n",
    "             (SELECT array_agg(id) FROM node_data) AS node_ids;\n",
    "    \"\"\")\n",
    "                results = cursor.fetchall()[0]\n",
    "                labels = np.array(results[1])\n",
    "                subgraph_node_features = np.array(results[0])\n",
    "                if results[0] is None:\n",
    "                    continue\n",
    "                \n",
    "                subgraph_edges = np.array(results[2]).transpose()\n",
    "                \n",
    "                node_ids = np.array(results[-1]) #subgraph_node_features[:, 0]\n",
    "                cols_source = np.searchsorted(node_ids, subgraph_edges[0])\n",
    "                cols_target = np.searchsorted(node_ids, subgraph_edges[1])\n",
    "                \n",
    "                remapped_edge_index = np.concatenate([np.expand_dims(cols_source, axis = 0), np.expand_dims(cols_target, axis = 0)], axis = 0)\n",
    "                features = subgraph_node_features #[:, 1:]\n",
    "                overall_run_time = time.time() - start \n",
    "                \n",
    "                complete_time += overall_run_time\n",
    "                # Testing\n",
    "                test_time = time.time()\n",
    "                remapped_edge_index_test, features_test, labels_test, unique_node_ids = get_subgraph_from_in_mem_graph_optimized(X, y, seed_node_id, edge_index, hops)                    \n",
    "                complete_test_time += time.time() - test_time\n",
    "                assert (sort_edge_index(torch.from_numpy(remapped_edge_index_test)) == sort_edge_index(torch.from_numpy(remapped_edge_index))).sum() / (remapped_edge_index_test.shape[-1] * remapped_edge_index_test.shape[0]), \"Edges doesnt match\"\n",
    "                assert np.allclose(node_ids, unique_node_ids)\n",
    "                assert np.max(np.abs(features_test - features)) < 1e-3, \"features does not match\"\n",
    "                assert np.allclose(labels_test, labels), \"Labels does not match\"\n",
    "                # print(f\"Fetched {remapped_edge_index.shape} edges, {labels.shape} labels, {features.shape} features in ({overall_run_time} s)\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                print(f\"Error reading data: {e}\")\n",
    "                raise   \n",
    "    return (complete_time, complete_test_time)\n",
    "\n",
    "def update_nodes(conn, X_and_y, random_sample_size = 1000):\n",
    "    feature_columns = list(filter(lambda col: \"f\" in col, X_and_y.columns))\n",
    "    label_columns = list(filter(lambda col: \"y\" in col, X_and_y.columns))\n",
    "    with conn.cursor() as cursor:\n",
    "        np.random.seed(42)\n",
    "        node_ids = np.random.choice(np.arange(X_and_y.shape[0]), size = random_sample_size, replace = False).tolist()\n",
    "        start = time.time()\n",
    "        for node_id in tqdm(node_ids):\n",
    "            np.random.seed(42)\n",
    "            features = np.random.rand(len(feature_columns)).tolist()  # Random values between 0 and 1\n",
    "            labels = np.random.randint(0, 2, size=len(label_columns)).tolist()  # Adjust label range as needed            \n",
    "            sql_query = f\"\"\"\n",
    "            UPDATE nodes \n",
    "            SET {\",\".join([f'{col} = %s' for col in feature_columns])}, {\",\".join([f'{col} = %s' for col in label_columns])}\n",
    "            WHERE id = %s;\n",
    "            \"\"\"\n",
    "            values = (*features, *labels, node_id)\n",
    "            cursor.execute(sql_query, values)\n",
    "        return time.time() - start\n",
    "\n",
    "def update_edges(conn, edge_index, X_and_y, random_sample_size = 1000):\n",
    "    with conn.cursor() as cursor:\n",
    "        np.random.seed(42)\n",
    "        edge_ids = np.random.choice(np.arange(edge_index.shape[-1]), size = random_sample_size, replace = False).tolist()\n",
    "        selected_edges = edge_index[:, edge_ids].transpose(-1, 0 )\n",
    "        start = time.time()\n",
    "        for selected_edge in tqdm(selected_edges):\n",
    "            source_id, target_id = selected_edge\n",
    "            np.random.seed(42)\n",
    "            new_target_id = int(np.random.randint(0, X_and_y.shape[0]))\n",
    "            sql_query = \"UPDATE edges SET target_id = %s WHERE source_id = %s AND target_id = %s;\"\n",
    "            values = (new_target_id, int(source_id), int(target_id))\n",
    "            cursor.execute(sql_query, values)\n",
    "        return time.time() - start\n",
    "\n",
    "def delete(conn, new_db_name):\n",
    "    start = time.time()\n",
    "    conn.close()\n",
    "    conn = connect_to_postgres()\n",
    "    delete_database(conn, new_db_name)\n",
    "    conn.close()\n",
    "    return time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eae14eb-d40c-4957-ac88-2aeefb10d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppi(output_df):\n",
    "    edge_file_name = \"ppi_edge_index.csv\"\n",
    "    node_file_name = \"ppi.csv\"\n",
    "    X = pd.read_csv(f\"data/ppi_x.csv\")\n",
    "    y = pd.read_csv(f\"data/ppi_y.csv\")\n",
    "    y.columns = [f\"y_{col}\" for col in y.columns]\n",
    "    y = y.astype(np.int8)\n",
    "    edges = pd.read_csv(\"data/\" + edge_file_name)\n",
    "    edges.columns = [\"source_id\", \"target_id\"]\n",
    "    X_and_y = X.copy()\n",
    "    X_and_y.columns = list(map(lambda col: f\"f_{col}\", X_and_y.columns))\n",
    "    X_and_y = pd.concat((X_and_y, y), axis = 1)\n",
    "    \n",
    "    node_file_name = \"X_y_ppi__postgres_columns.csv\"\n",
    "    X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \",\", index = True)\n",
    "    edges.to_csv(f\"syn_data/{edge_file_name}\", sep = \",\", index = False)\n",
    "    edge_index = edges.values.transpose(-1, 0)\n",
    "    \n",
    "    conn, db_name = create_db(node_file_name)\n",
    "    create_time = create(conn, node_file_name, edge_file_name, X_and_y)\n",
    "    read_times, read_times_mem = dict(), dict()\n",
    "    for hops in range(1,4):\n",
    "        read_time, read_time_mem = read(conn, hops, X_and_y, X, y, edge_index, random_sample_size = 1_000)\n",
    "        read_times[hops] = read_time\n",
    "        read_times_mem[hops] = read_time_mem\n",
    "        \n",
    "    update_time_nodes = update_nodes(conn, X_and_y, 1_000)\n",
    "    update_time_edges = update_edges(conn, edge_index, X_and_y, 1_000)\n",
    "    delete_time= delete(conn,db_name )\n",
    "    new_row_dict = {\"name\": \"ppi\", \"create\": create_time, \"update_nodes\": update_time_nodes, \"update_edges\": update_time_edges, \"delete\": delete_time}\n",
    "    for hops in read_times:\n",
    "        new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "        new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "    new_row = pd.DataFrame([new_row_dict])\n",
    "    output_df = pd.concat((output_df, new_row), ignore_index=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b909a3-3238-481d-87c5-8204871717de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_synth(output_df):\n",
    "    for num_nodes in tqdm([1_000, 10_000, 100_000, 1_000_000]):\n",
    "        for num_edges in tqdm([\"5_edges\", \"10_edges\", \"20_edges\", \"scale_free\"]):\n",
    "            if num_nodes == 1_000_000 and (num_edges == \"10_edges\" or num_edges == \"20_edges\"): continue\n",
    "            feature_file_name = f\"X_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            label_file_name = f\"y_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            edge_file_name = f\"edge_index_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            assert os.path.exists(f\"syn_data/{feature_file_name}\"), \"Feature file does not exist\"\n",
    "            assert os.path.exists(f\"syn_data/{label_file_name}\"), \"Label file does not exist\"\n",
    "            assert os.path.exists(f\"syn_data/{edge_file_name}\"), \"Edge file does not exist\"\n",
    "            \n",
    "            X = pd.read_csv(f\"syn_data/{feature_file_name}\")\n",
    "            y = pd.read_csv(f\"syn_data/{label_file_name}\")\n",
    "            y.columns = [f\"y_{col}\" for col in y.columns]\n",
    "            y = y.astype(np.int8)\n",
    "            edges = pd.read_csv(f\"syn_data/{edge_file_name}\")\n",
    "            edges.columns = [\"source_id\", \"target_id\"]\n",
    "            edge_index = edges.values.transpose(-1, 0)\n",
    "            node_file_name = f\"X_and_y_{str(num_nodes)}_nodes_{num_edges}_postgres_columns.csv\"\n",
    "            if not os.path.exists(f\"syn_data/{node_file_name}\"):\n",
    "                X_and_y = X.copy()\n",
    "                X_and_y.columns = list(map(lambda col: f\"f_{col}\", X_and_y.columns))\n",
    "                X_and_y = pd.concat((X_and_y, y), axis = 1)\n",
    "                X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \",\", index = True)\n",
    "            else:\n",
    "                X_and_y = pd.read_csv(f\"syn_data/{node_file_name}\", sep = \",\", index_col = 0)\n",
    "            conn, new_db_name = create_db(node_file_name)\n",
    "            create_time = create(conn, node_file_name, edge_file_name, X_and_y)\n",
    "            read_times = dict()\n",
    "            read_times_mem = dict()\n",
    "            for hops in tqdm(range(1, 4)):\n",
    "                read_time, read_time_mem = read(conn, hops, X_and_y, X, y, edge_index, 1_000)\n",
    "                read_times[hops] = read_time\n",
    "                read_times_mem[hops] = read_time_mem\n",
    "            update_time_nodes = update_nodes(conn, X_and_y, 1000)\n",
    "            update_time_edges = update_edges(conn, edge_index, X_and_y, 1000)\n",
    "            delete_time = delete(conn, new_db_name)\n",
    "            new_row_dict = {\"name\": f\"{str(num_nodes)}_nodes_{num_edges}\", \"create\": create_time, \"update_nodes\": update_time_nodes, \"update_edges\": update_time_edges, \"delete\": delete_time}\n",
    "            for hops in read_times:\n",
    "                new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "                new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "            new_row = pd.DataFrame([new_row_dict])\n",
    "            output_df = pd.concat((output_df, new_row), ignore_index=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000d47ce-f362-4225-a3cb-b6aca984a701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37663315004c4812a64d4262dd125e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa44f34dfa67420bbe463fa4fc39b53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n",
      "Database 'X_and_y_1000_nodes_5_edges_postgres_columns' created successfully.\n",
      "Connection successful.\n",
      "Schema created successfully.\n",
      "Edges table created successfully.\n",
      "Data from 'syn_data/X_and_y_1000_nodes_5_edges_postgres_columns.csv' uploaded to table 'nodes' successfully using COPY.\n",
      "Index 'nodes_id_idx' created successfully.\n",
      "Edges data from 'syn_data/edge_index_1000_nodes_5_edges.csv' uploaded successfully.\n",
      "Index 'edges_target_id_idx' created successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5037f1ab54ac43d2a7a7aa7a7766600f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17990c0a6834a6a99b5fa317bf96b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e32875bace445d852e326a717c6baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb6f528df6f4317afe6ebcb97c146f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7f9e88c696c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dwalke/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_edges\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# output_df = eval_ppi(output_df)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m output_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_synth\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m output_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres_col_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36meval_synth\u001b[0;34m(output_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m read_times_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hops \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)):\n\u001b[0;32m---> 32\u001b[0m     read_time, read_time_mem \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_and_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     read_times[hops] \u001b[38;5;241m=\u001b[39m read_time\n\u001b[1;32m     34\u001b[0m     read_times_mem[hops] \u001b[38;5;241m=\u001b[39m read_time_mem\n",
      "Cell \u001b[0;32mIn[2], line 261\u001b[0m, in \u001b[0;36mread\u001b[0;34m(conn, hops, X_and_y, X, y, edge_index, random_sample_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m             start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 261\u001b[0m             \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;43m    WITH RECURSIVE NestedTargets AS (\u001b[39;49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;43m        SELECT 0 AS depth, source_id, target_id\u001b[39;49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;43m        FROM edges\u001b[39;49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;43m        WHERE target_id = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed_node_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;43m        UNION ALL\u001b[39;49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;43m        SELECT nt.depth + 1, e.source_id, e.target_id\u001b[39;49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;43m        FROM edges e\u001b[39;49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;43m        JOIN NestedTargets nt ON e.target_id = nt.source_id\u001b[39;49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;43m        WHERE nt.depth < \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhops\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;43m    ),\u001b[39;49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;43m    node_ids AS (\u001b[39;49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;43m        SELECT DISTINCT id FROM (\u001b[39;49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;43m            SELECT source_id AS id FROM NestedTargets\u001b[39;49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;43m            UNION\u001b[39;49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;43m            SELECT target_id AS id FROM NestedTargets\u001b[39;49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;43m        ) AS combined_ids\u001b[39;49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;43m    ),\u001b[39;49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;43m    node_data AS (\u001b[39;49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;43m        SELECT \u001b[39;49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;43m            id, \u001b[39;49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_and_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;43m        FROM nodes\u001b[39;49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;43m        WHERE id IN (SELECT id FROM node_ids)\u001b[39;49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;43m        ORDER BY id\u001b[39;49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;43m    )\u001b[39;49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;43m    SELECT\u001b[39;49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;43m        (SELECT array_agg(array[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]) FROM node_data) AS node_table,\u001b[39;49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;43m        (SELECT array_agg(array[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]) FROM node_data) AS label_table,\u001b[39;49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;43m        (SELECT array_agg(array[source_id, target_id])\u001b[39;49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;43m         FROM (SELECT DISTINCT source_id, target_id FROM NestedTargets) AS edges) AS edge_table,\u001b[39;49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;43m         (SELECT array_agg(id) FROM node_data) AS node_ids;\u001b[39;49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m             results \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchall()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    300\u001b[0m             labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.10/encodings/utf_8.py:15\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m### Codec APIs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m encode \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mutf_8_encode\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28minput\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mutf_8_decode(\u001b[38;5;28minput\u001b[39m, errors, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalEncoder\u001b[39;00m(codecs\u001b[38;5;241m.\u001b[39mIncrementalEncoder):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 1\n",
    "off_set = 0\n",
    "for i in range(off_set, num_iterations + off_set):\n",
    "    print(f\"Iteration {i}\")\n",
    "    output_df = pd.DataFrame(columns = [\"name\", \"create\", \"update_nodes\", \"update_edges\", \"delete\"])\n",
    "    # output_df = eval_ppi(output_df)\n",
    "    output_df = eval_synth(output_df)\n",
    "    output_df.to_csv(f\"postgres_col_{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890eb50-d465-49eb-8275-0fc00a522251",
   "metadata": {},
   "source": [
    "#### Okay, for some really weird reasin without in-database ordering by id postgres seems to randomizes feature ordering (potentially some interaction with array_agg? - I have no clue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0542cfed-eb0d-4baa-95b5-346de684b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n",
      "Connection successful.\n",
      "Database 'X_and_y_1000000_nodes_scale_free_postgres_columns' deleted successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23974990844726562"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = connect_to_postgres()\n",
    "delete(conn, \"X_and_y_1000000_nodes_scale_free_postgres_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ea9e7c-1c91-4f01-b5d6-7f5f5ffccb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_1(scale):\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "    a = np.expand_dims(np.random.rand(500*scale, 1), axis = 0)\n",
    "    b = np.expand_dims(np.random.rand(500*scale, 1), axis = 0)\n",
    "    print(a.shape)\n",
    "    return np.isclose(a,b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7882cc0f-9e7d-4e1e-91da-e65e1d9864d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([17.501884])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tracemalloc\n",
    "import numpy as np\n",
    "tracemalloc.start()\n",
    "fun_1(1000)\n",
    "mem = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "np.diff(np.array(mem)) / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fceb92a-3c6e-4350-81d2-f0f80d505b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 11302)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array([87500.001832])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee900d-5d1a-480a-9d7d-31a6e18d9444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
