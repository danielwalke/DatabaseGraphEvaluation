{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec6be63-14a1-4e9a-b9ad-4fa142097f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import docker\n",
    "import os\n",
    "import neo4j\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90361ec5-f0fd-43e7-b454-0bbb2e764302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neo4j_container():\n",
    "\tclient = docker.from_env()\n",
    "\n",
    "\t# Define the image name\n",
    "\timage_name = \"neo4j:latest\"\n",
    "\n",
    "\t# Define the container name and environment variables\n",
    "\tcontainer_name = \"neo4j-apoc\"\n",
    "\tneo4j_env = {\n",
    "\t\t\"NEO4J_apoc_export_file_enabled\": \"true\",\n",
    "\t\t\"NEO4J_apoc_import_file_enabled\": \"true\",\n",
    "\t\t\"NEO4J_apoc_import_file_use__neo4j__config\": \"true\",\n",
    "\t\t\"NEO4J_PLUGINS\": '[\"apoc\"]',\n",
    "\t\t\"NEO4J_AUTH\": \"neo4j/password\",\n",
    "\t}\n",
    "\n",
    "\t# Define the volume mapping\n",
    "\tvolume_mapping = {\n",
    "\t\tf\"{os.getcwd()}/neo4j_data\": {\"bind\": \"/data\", \"mode\": \"rw\"},\n",
    "\t\tf\"{os.getcwd()}/syn_data\": {\"bind\": \"/import\", \"mode\": \"rw\"}\n",
    "\t}\n",
    "\tports = {\"7474\": 7475, \"7687\": 7688}\n",
    "\tif len(client.containers.list(filters = {'name' : 'neo4j-apoc'})) == 0:\n",
    "\t\tcontainer = client.containers.run(\n",
    "\t\t\timage=image_name,\n",
    "\t\t\tname=container_name,\n",
    "\t\t\tports=ports,\n",
    "\t\t\tenvironment=neo4j_env,\n",
    "\t\t\tvolumes=volume_mapping,\n",
    "\t\t\tdetach=True,\n",
    "\t\t\ttty=True,\n",
    "\t\t\tstdin_open=True,\n",
    "\t\t\tremove=True,  # Automatically remove the container when it stops\n",
    "\t\t)\n",
    "\t\tsleep(5)\n",
    "\t\tprint(f\"Container {container_name} started with ID: {container.id}\")\n",
    "\telse:\n",
    "\t\tprint(\"Container neo4j-apoc already exists\")\n",
    "\t\tcontainer = client.containers.get('neo4j-apoc')\n",
    "\n",
    "\tmax_timeout = 300  # Maximum wait time in seconds (5 minutes)\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\turi = f\"bolt://localhost:{ports['7687']}\"\n",
    "\tusername = \"neo4j\"\n",
    "\tpassword = \"password\"\n",
    "\tprint(\"Waiting for Neo4j to start...\")\n",
    "\twhile True:\n",
    "\t\ttry:\n",
    "\t\t\tdriver = connect_to_neo4j(uri, username, password)\n",
    "\t\t\tdriver.execute_query(\"MATCH (n) RETURN n\")\n",
    "\t\t\tprint(\"Neo4j is up and running.\")\n",
    "\t\t\treturn driver, container\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"Waiting for neo4j startup\")\n",
    "\t\t\tif start_time > max_timeout:\n",
    "\t\t\t\tbreak\n",
    "\t\tsleep(5)\n",
    "\treturn None, container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e04f121-dc04-4469-80ac-e1a98881724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container neo4j-apoc started with ID: 6c342511b524a8a08fdc6367f6976aa01f577930d51ec21dde3cf246c4a58b0a\n",
      "Waiting for Neo4j to start...\n",
      "Waiting for neo4j startup\n"
     ]
    }
   ],
   "source": [
    "container, ports = create_neo4j_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e235e01-006f-4b3a-bc47-7439374f4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#container.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05af57f-5f07-4ef3-aef9-8d018db8c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_neo4j(uri, user, password):\n",
    "    \"\"\"\n",
    "    Establishes a connection to the Neo4j database.\n",
    "\n",
    "    Args:\n",
    "        uri (str): The URI of the Neo4j database.\n",
    "        user (str): The username for authentication.\n",
    "        password (str): The password for authentication.\n",
    "\n",
    "    Returns:\n",
    "        GraphDatabase.driver: The Neo4j driver instance for the connection.\n",
    "    \"\"\"\n",
    "\n",
    "    return GraphDatabase.driver(uri, auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f64df0-bbcd-4a19-a4ed-82ef0c58785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transaction failed and will be retried in 0.9912263515758662s (Couldn't connect to localhost:7688 (resolved to ()):\n",
      "Connection to 127.0.0.1:7688 closed without handshake response)\n",
      "Transaction failed and will be retried in 2.0965165452019154s (Couldn't connect to localhost:7688 (resolved to ()):\n",
      "Failed to read any data from server ResolvedIPv4Address(('127.0.0.1', 7688)) after connected (deadline Deadline(timeout=60.0)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[], summary=<neo4j._work.summary.ResultSummary object at 0x7efdd04bf760>, keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uri = f\"bolt://localhost:{ports['7687']}\"\n",
    "uri = f\"bolt://localhost:{7688}\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "driver = connect_to_neo4j(uri, username, password)\n",
    "driver.execute_query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d031907-f712-4079-ab85-d33fced4e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transaction failed and will be retried in 0.8479369836776482s (The allocation of an extra 1.5 GiB would use more than the limit 20.6 GiB. Currently using 19.2 GiB. dbms.memory.transaction.total.max threshold reached)\n"
     ]
    },
    {
     "ename": "TransientError",
     "evalue": "{code: Neo.TransientError.General.MemoryPoolOutOfMemoryError} {message: The allocation of an extra 1.5 GiB would use more than the limit 20.6 GiB. Currently using 19.2 GiB. dbms.memory.transaction.total.max threshold reached}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTransientError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMATCH (n) DETACH DELETE n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/driver.py:902\u001b[0m, in \u001b[0;36mDriver.execute_query\u001b[0;34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid routing control value: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    900\u001b[0m                      \u001b[38;5;241m%\u001b[39m routing_)\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_pipelined_begin:\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDRIVER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_transformer_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/work/session.py:588\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransaction failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/work/session.py:564\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m         \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_commit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DriverError, Neo4jError) \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/work/transaction.py:186\u001b[0m, in \u001b[0;36mTransactionBase._commit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mcommit(on_success\u001b[38;5;241m=\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bookmark \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbookmark\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_database \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_database)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/io/_bolt.py:863\u001b[0m, in \u001b[0;36mBolt.fetch_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcomplete:\n\u001b[0;32m--> 863\u001b[0m     detail_delta, summary_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     detail_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m detail_delta\n\u001b[1;32m    865\u001b[0m     summary_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary_delta\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/io/_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    848\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[0;31mTransientError\u001b[0m: {code: Neo.TransientError.General.MemoryPoolOutOfMemoryError} {message: The allocation of an extra 1.5 GiB would use more than the limit 20.6 GiB. Currently using 19.2 GiB. dbms.memory.transaction.total.max threshold reached}"
     ]
    }
   ],
   "source": [
    "driver.execute_query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ce56d7-4ef7-43d8-acf9-d51b664e6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_edge_index_iterative(source_nodes, edge_index, max_depth):\n",
    "    \"\"\"\n",
    "    Optimized function to compute the subgraph around the source nodes up to a given depth.\n",
    "    Uses an iterative approach instead of recursion.\n",
    "    \"\"\"\n",
    "    visited_nodes = set(source_nodes)\n",
    "    current_frontier = np.array(source_nodes)\n",
    "    \n",
    "    subgraph_edges = []\n",
    "\n",
    "    for _ in range(max_depth):\n",
    "        # Find edges where the target node is in the current frontier\n",
    "        target_mask = np.isin(edge_index[1], current_frontier)\n",
    "        subgraph_edge_index = edge_index[:, target_mask]\n",
    "        subgraph_edges.append(subgraph_edge_index)\n",
    "\n",
    "        # Update the current frontier with the source nodes of these edges\n",
    "        current_frontier = np.setdiff1d(subgraph_edge_index[0], list(visited_nodes))\n",
    "        visited_nodes.update(current_frontier)\n",
    "        \n",
    "        if len(current_frontier) == 0:\n",
    "            break\n",
    "\n",
    "    # Combine edges from all hops\n",
    "    return np.concatenate(subgraph_edges, axis=1) if subgraph_edges else np.empty((2, 0), dtype=edge_index.dtype)\n",
    "\n",
    "\n",
    "def get_subgraph_from_in_mem_graph_optimized(X, y, i, edge_index, hops):\n",
    "    \"\"\"\n",
    "    Optimized version of subgraph extraction.\n",
    "    \"\"\"\n",
    "    subgraph_edge_index = recurse_edge_index_iterative([i], edge_index, hops)\n",
    "    unique_node_ids, remapping = np.unique(subgraph_edge_index, return_inverse=True)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = X.iloc[unique_node_ids, :].values\n",
    "    labels = y.iloc[unique_node_ids, :].values.squeeze()\n",
    "\n",
    "    # Remap edge indices\n",
    "    remapped_edge_index = remapping.reshape(2, -1)\n",
    "    return remapped_edge_index, features, labels, unique_node_ids\n",
    "    \n",
    "def create(driver, X_and_y, node_file_name, edge_file_name):\n",
    "    label_columns = list(filter(lambda col: \"y\" in col, X_and_y.columns))\n",
    "    feature_columns = list(filter(lambda col: \"f\" in col, X_and_y.columns))\n",
    "    label_columns_setter = \"\\n\".join([f\"SET p['{col}'] = toInteger(line['{col}'])\" for col in label_columns])\n",
    "    feature_columns_setter = \"\\n\".join([f\"SET p['{col}'] = toFloat(line['{col}'])\" for col in feature_columns])\n",
    "    with driver.session() as session:\n",
    "        start = time.time()\n",
    "        node_query = f\"\"\"\n",
    "                LOAD CSV WITH HEADERS FROM $file AS line\n",
    "                WITH line, linenumber() AS index\n",
    "                CALL (line, index) {{\n",
    "                  MERGE (p:Node {{id: index - 2}})\n",
    "                  {label_columns_setter}\n",
    "                  {feature_columns_setter}\n",
    "                }} IN TRANSACTIONS OF 10000 ROWS\n",
    "                \"\"\"\n",
    "        session.run(node_query, file=f\"file:///{node_file_name}\")\n",
    "        session.run(\"CREATE INDEX IF NOT EXISTS FOR (n:Node) ON (n.id)\")\n",
    "        edge_query = \"\"\"\n",
    "            LOAD CSV FROM $file AS line\n",
    "            WITH line, linenumber() AS index\n",
    "            WHERE index <> 1\n",
    "            CALL (line) {\n",
    "                MATCH (s:Node {id: toInteger(line[0])}), (t:Node {id: toInteger(line[1])})\n",
    "                CREATE (s)-[r:connects]->(t)\n",
    "            } IN TRANSACTIONS OF 10000 ROWS\n",
    "            \"\"\"\n",
    "        session.run(edge_query, file=f\"file:///{edge_file_name}\")\n",
    "        return time.time() - start\n",
    "        \n",
    "def read(driver, X_and_y, hops, X, y, edge_index, random_sample_size= 1000):\n",
    "    overall_run_time = 0     \n",
    "    test_time = 0     \n",
    "    label_columns = list(filter(lambda col: \"y\" in col, X_and_y.columns))\n",
    "    feature_columns = list(filter(lambda col: \"f\" in col, X_and_y.columns))\n",
    "    feature_columns_getter = \",\".join([f\"node['{col}']\" for col in feature_columns])\n",
    "    feature_columns_getter = f'[{feature_columns_getter}] as nodeFeatures'\n",
    "    label_columns_getter = \",\".join([f\"node['{col}']\" for col in label_columns])\n",
    "    label_columns_getter = f'[{label_columns_getter}] as nodeLabels'\n",
    "    with driver.session(database = \"neo4j\") as session:\n",
    "        np.random.seed(42)\n",
    "        seed_node_ids = np.random.choice(np.arange(X_and_y.shape[0]), size = random_sample_size, replace = False)\n",
    "        for seed_node_id in tqdm(seed_node_ids):\n",
    "            start = time.time()\n",
    "            subgraph_query = f\"\"\"\n",
    "                MATCH (t {{id: $seed_node_id}})\n",
    "                MATCH (s)-[r*0..{hops}]->(t)\n",
    "                UNWIND r AS edge\n",
    "                WITH t, edge\n",
    "                WITH \n",
    "                  collect(DISTINCT [startNode(edge).id, endNode(edge).id]) AS edges,\n",
    "                  collect(DISTINCT startNode(edge)) AS startNodes,\n",
    "                  t AS endNode\n",
    "                UNWIND (startNodes + [endNode]) AS node\n",
    "                WITH edges, COLLECT(DISTINCT node) AS uniqueNodes\n",
    "                UNWIND uniqueNodes AS node\n",
    "                WITH edges, node.id as nodeId, {feature_columns_getter}, {label_columns_getter}\n",
    "                ORDER BY nodeId\n",
    "                RETURN \n",
    "                  edges,\n",
    "                  collect(nodeId) AS idCollection,\n",
    "                  collect(nodeLabels) AS labels,\n",
    "                  collect(nodeFeatures) AS features\n",
    "                \"\"\"\n",
    "            ## ORDER BY nodeId seems to increase speed \n",
    "            results = session.run(subgraph_query, seed_node_id = seed_node_id)\n",
    "            res_df = results.to_df()\n",
    "            if res_df.empty: continue\n",
    "            subgraph_edge_index = np.array(res_df[\"edges\"][0]).transpose()        \n",
    "            node_ids = np.array(res_df[\"idCollection\"][0])\n",
    "            # id_sort_idx = np.argsort(node_ids)\n",
    "            # node_ids = node_ids[id_sort_idx]\n",
    "            cols_source = np.searchsorted(node_ids, subgraph_edge_index[0])\n",
    "            cols_target = np.searchsorted(node_ids, subgraph_edge_index[1])\n",
    "            \n",
    "            remapped_edge_index = np.concatenate([np.expand_dims(cols_source, axis = 0), np.expand_dims(cols_target, axis = 0)], axis = 0)\n",
    "            features =np.array(res_df[\"features\"][0]) #[id_sort_idx]\n",
    "            labels = np.array(res_df[\"labels\"][0]) #[id_sort_idx]   \n",
    "            overall_run_time += time.time() - start    \n",
    "            \n",
    "            ## Testing\n",
    "            start = time.time()\n",
    "            remapped_edge_index_test, features_test, labels_test, unique_node_ids = get_subgraph_from_in_mem_graph_optimized(X, y, seed_node_id, edge_index, hops)        \n",
    "            test_time += time.time() - start\n",
    "            assert (sort_edge_index(torch.from_numpy(remapped_edge_index_test)) == sort_edge_index(torch.from_numpy(remapped_edge_index))).sum() / (remapped_edge_index_test.shape[-1] * remapped_edge_index_test.shape[0]), \"Edges doesnt match\"\n",
    "            del remapped_edge_index_test\n",
    "            del remapped_edge_index\n",
    "            gc.collect()\n",
    "            assert np.allclose(node_ids, unique_node_ids), \"Node ids does not match\"\n",
    "            del node_ids\n",
    "            del unique_node_ids\n",
    "            gc.collect()\n",
    "            assert np.allclose(features.astype(np.float32), features_test.astype(np.float32)), \"features does not match\"\n",
    "            del features\n",
    "            del features_test\n",
    "            gc.collect()\n",
    "            assert np.allclose(labels_test, labels), \"Labels does not match\"\n",
    "            del labels\n",
    "            del labels_test\n",
    "            gc.collect()\n",
    "        return overall_run_time, test_time\n",
    "\n",
    "def update_nodes(driver, X_and_y, random_sample_size = 1000):\n",
    "    feature_columns = list(filter(lambda col: \"f\" in col, X_and_y.columns))\n",
    "    label_columns = list(filter(lambda col: \"y\" in col, X_and_y.columns))\n",
    "    with driver.session(database = \"neo4j\") as session:\n",
    "        np.random.seed(42)\n",
    "        node_ids = np.random.choice(np.arange(X_and_y.shape[0]), size = random_sample_size, replace = False).tolist()\n",
    "        overall_run_time = 0\n",
    "        for node_id in tqdm(node_ids):\n",
    "            np.random.seed(42)\n",
    "            features = np.random.rand(len(feature_columns)).tolist()  # Random values between 0 and 1\n",
    "            labels = np.random.randint(0, 2, size=len(label_columns)).tolist()  # Adjust label range as needed  \n",
    "            label_columns_setter = \"\\n\".join([f\"SET n['{col}'] = {int(labels[i])}\" for i, col in enumerate(label_columns)])\n",
    "            feature_columns_setter = \"\\n\".join([f\"SET n['{col}'] = {features[i]}\" for i, col in enumerate(feature_columns)])\n",
    "            start = time.time()\n",
    "            cypher_query = f\"\"\"\n",
    "            MATCH (n:Node {{id: $node_id}})\n",
    "            {label_columns_setter}\n",
    "            {feature_columns_setter}\n",
    "            \"\"\"\n",
    "            session.run(cypher_query, node_id=node_id)\n",
    "            overall_run_time += time.time() - start\n",
    "        return overall_run_time\n",
    "\n",
    "def update_edges(driver, edge_index, X_and_y, random_sample_size = 1000):\n",
    "    np.random.seed(42)\n",
    "    edge_ids = np.random.choice(np.arange(edge_index.shape[-1]),\n",
    "                                size=random_sample_size,\n",
    "                                replace=False).tolist()\n",
    "    selected_edges = edge_index[:, edge_ids].transpose(-1, 0)\n",
    "    with driver.session() as session:\n",
    "        start = time.time()\n",
    "        for selected_edge in tqdm(selected_edges, desc=\"Updating edges\"):\n",
    "            source_id, target_id = selected_edge\n",
    "            np.random.seed(42)\n",
    "            new_target_id = int(np.random.randint(0, X_and_y.shape[0]))\n",
    "            \n",
    "            # Cypher query to update the target_id of the relationship that matches both source_id and target_id.\n",
    "            cypher_query = \"\"\"\n",
    "            MATCH (s {id: $source_id}), (t {id: $target_id}), (new_t {id: $new_target_id})\n",
    "            MATCH (s)-[r:connects]->(t)\n",
    "            DELETE r\n",
    "            CREATE (s)-[new_r:connects]->(new_t)\n",
    "            \"\"\"\n",
    "            session.run(cypher_query,\n",
    "                        source_id=int(source_id),\n",
    "                        target_id=int(target_id),\n",
    "                        new_target_id=new_target_id)\n",
    "    \n",
    "    return time.time() - start\n",
    "    \n",
    "\n",
    "def delete(driver):\n",
    "    start = time.time()\n",
    "    driver.execute_query(\"MATCH (n) DETACH DELETE(n)\")\n",
    "    return time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa879ce-3dea-494f-89b0-754b3a68e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edge_file_name = \"ppi_edge_index.csv\"\n",
    "node_file_name = \"ppi.csv\"\n",
    "X = pd.read_csv(f\"data/ppi_x.csv\")\n",
    "y = pd.read_csv(f\"data/ppi_y.csv\")\n",
    "edges = pd.read_csv(\"data/\" + edge_file_name)\n",
    "edges.columns = [\"source_id\", \"target_id\"]\n",
    "\n",
    "X.columns = list(map(lambda col: f\"f_{col}\", X.columns))\n",
    "y.columns = list(map(lambda col: f\"y_{col}\", y.columns))\n",
    "y = y.astype(np.int8)\n",
    "X_and_y = X.copy()\n",
    "X_and_y = pd.concat((X_and_y, y), axis = 1)\n",
    "\n",
    "node_file_name = \"X_y_ppi_col.csv\" ## Can use the same structure for all database when using column wise import\n",
    "X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \",\", index = True)\n",
    "edges.to_csv(f\"syn_data/{edge_file_name}\", sep = \",\", index = False)\n",
    "edge_index = edges.values.transpose(-1, 0)\n",
    "create_time = create(driver, X_and_y, node_file_name, edge_file_name)\n",
    "read_times = dict()\n",
    "read_times_mem = dict()\n",
    "for hops in tqdm(range(1, 4)):\n",
    "    read_time, read_time_mem = read(driver, X_and_y, hops, X, y, edge_index, random_sample_size= 1000)\n",
    "    read_times[hops] = read_time\n",
    "    read_times_mem[hops] = read_time_mem\n",
    "update_time_nodes = update_nodes(driver, X_and_y, random_sample_size = 1000)\n",
    "update_time_edges = update_edges(driver, edge_index, X_and_y, random_sample_size = 1000)\n",
    "delete_time = delete(driver)\n",
    "new_row_dict = {\"name\": \"PPI\", \"create\": create_time, \"update_nodes\": update_node_time, \"update_edges\": update_edge_time, \"delete\": delete_time}\n",
    "for hops in read_times:\n",
    "    new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "    new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "new_row = pd.DataFrame([new_row_dict])\n",
    "output_df = pd.concat((output_df, new_row), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8e00e37-d41a-488e-88dc-9bcd915c3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppi(output_df):\n",
    "    edge_file_name = \"ppi_edge_index.csv\"\n",
    "    node_file_name = \"ppi.csv\"\n",
    "    X = pd.read_csv(f\"data/ppi_x.csv\")\n",
    "    y = pd.read_csv(f\"data/ppi_y.csv\")\n",
    "    edges = pd.read_csv(\"data/\" + edge_file_name)\n",
    "    edges.columns = [\"source_id\", \"target_id\"]\n",
    "\n",
    "    X.columns = list(map(lambda col: f\"f_{col}\", X.columns))\n",
    "    y.columns = list(map(lambda col: f\"y_{col}\", y.columns))\n",
    "    y = y.astype(np.int8)\n",
    "    X_and_y = X.copy()\n",
    "    X_and_y = pd.concat((X_and_y, y), axis = 1)\n",
    "    \n",
    "    node_file_name = \"X_y_ppi_neo4j_col.csv\" ## Can use the same structure for all database when using column wise import\n",
    "    X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \",\", index = True)\n",
    "    edges.to_csv(f\"syn_data/{edge_file_name}\", sep = \",\", index = False)\n",
    "    edge_index = edges.values.transpose(-1, 0)\n",
    "    create_time = create(driver, X_and_y, node_file_name, edge_file_name)\n",
    "    read_times = dict()\n",
    "    read_times_mem = dict()\n",
    "    for hops in tqdm(range(1, 4)):\n",
    "        read_time, read_time_mem = read(driver, X_and_y, hops, X, y, edge_index, random_sample_size= 1000)\n",
    "        read_times[hops] = read_time\n",
    "        read_times_mem[hops] = read_time_mem\n",
    "    update_time_nodes = update_nodes(driver, X_and_y, random_sample_size = 1000)\n",
    "    update_time_edges = update_edges(driver, edge_index, X_and_y, random_sample_size = 1000)\n",
    "    delete_time = delete(driver)\n",
    "    new_row_dict = {\"name\": \"PPI\", \"create\": create_time, \"update_nodes\": update_node_time, \"update_edges\": update_edge_time, \"delete\": delete_time}\n",
    "    for hops in read_times:\n",
    "        new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "        new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "    new_row = pd.DataFrame([new_row_dict])\n",
    "    output_df = pd.concat((output_df, new_row), ignore_index=True)\n",
    "    return output_df\n",
    "\t\n",
    "def eval_synth(output_df):\n",
    "    for num_nodes in tqdm([1_000, 10_000, 100_000, 1_000_000]):\n",
    "        for num_edges in tqdm([\"5_edges\", \"10_edges\", \"20_edges\", \"scale_free\"]):\n",
    "            if num_nodes == 1_000_000 and (num_edges == \"10_edges\" or num_edges == \"20_edges\"): continue\n",
    "            feature_file_name = f\"X_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            label_file_name = f\"y_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            edge_file_name = f\"edge_index_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "            assert os.path.exists(f\"syn_data/{feature_file_name}\"), \"Feature file does not exist\"\n",
    "            assert os.path.exists(f\"syn_data/{label_file_name}\"), \"Label file does not exist\"\n",
    "            assert os.path.exists(f\"syn_data/{edge_file_name}\"), \"Edge file does not exist\"\n",
    "            \n",
    "            X = pd.read_csv(f\"syn_data/{feature_file_name}\")\n",
    "            X.columns = list(map(lambda col: f\"f_{col}\", X.columns))\n",
    "            y = pd.read_csv(f\"syn_data/{label_file_name}\")\n",
    "            y.columns = list(map(lambda col: f\"y_{col}\", y.columns))\n",
    "            y = y.astype(np.int8)\n",
    "            \n",
    "            edges = pd.read_csv(f\"syn_data/{edge_file_name}\")\n",
    "            edges.columns = [\"source_id\", \"target_id\"]\n",
    "            edge_index = edges.values.transpose(-1, 0)\n",
    "            node_file_name = f\"X_and_y_{str(num_nodes)}_nodes_{num_edges}_neo4j_col.csv\"\n",
    "            if True or not os.path.exists(f\"syn_data/{node_file_name}\"):\n",
    "                X_and_y = X.copy()\n",
    "                X_and_y = pd.concat((X_and_y, y), axis = 1)\n",
    "                X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \",\", index = True)\n",
    "            else:\n",
    "                X_and_y = pd.read_csv(f\"syn_data/{node_file_name}\", sep = \";\", index_col = 0)\n",
    "            create_time = create(driver, X_and_y, node_file_name, edge_file_name)\n",
    "            read_times = dict()\n",
    "            read_times_mem = dict()\n",
    "            for hops in tqdm(range(1, 4)):\n",
    "                read_time, read_time_mem = read(driver, X_and_y, hops, X, y, edge_index, random_sample_size= 1000)\n",
    "                read_times[hops] = read_time\n",
    "                read_times_mem[hops] = read_time_mem\n",
    "            update_time_nodes = update_nodes(driver, X_and_y, random_sample_size = 1000)\n",
    "            update_time_edges = update_edges(driver, edge_index, X_and_y, random_sample_size = 1000)\n",
    "            delete_time = delete(driver)\n",
    "            new_row_dict = {\"name\": f\"{str(num_nodes)}_nodes_{num_edges}\", \"create\": create_time, \"update_nodes\": update_time_nodes, \"update_edges\": update_time_edges, \"delete\": delete_time}\n",
    "            for hops in read_times:\n",
    "                new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "                new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "            new_row = pd.DataFrame([new_row_dict])\n",
    "            output_df = pd.concat((output_df, new_row), ignore_index=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cfe14dd-1923-4e61-9517-7b0af4e7120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b54c3614e34a1fb907e7e5f27f3b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04d078f2ca0419f83a7a66d9a698648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_edges\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m output_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppi\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m output_df \u001b[38;5;241m=\u001b[39m eval_synth(output_df)\n\u001b[1;32m      8\u001b[0m output_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j_col_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m, in \u001b[0;36meval_ppi\u001b[0;34m(output_df)\u001b[0m\n\u001b[1;32m     21\u001b[0m read_times_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hops \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)):\n\u001b[0;32m---> 23\u001b[0m     read_time, read_time_mem \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_and_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     read_times[hops] \u001b[38;5;241m=\u001b[39m read_time\n\u001b[1;32m     25\u001b[0m     read_times_mem[hops] \u001b[38;5;241m=\u001b[39m read_time_mem\n",
      "Cell \u001b[0;32mIn[21], line 134\u001b[0m, in \u001b[0;36mread\u001b[0;34m(driver, X_and_y, hops, X, y, edge_index, random_sample_size)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m unique_node_ids\n\u001b[1;32m    133\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_test\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m features\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m features_test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/numeric.py:2241\u001b[0m, in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_allclose_dispatcher)\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallclose\u001b[39m(a, b, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-8\u001b[39m, equal_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2173\u001b[0m \u001b[38;5;124;03m    Returns True if two arrays are element-wise equal within a tolerance.\u001b[39;00m\n\u001b[1;32m   2174\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2239\u001b[0m \n\u001b[1;32m   2240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2241\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(res)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/numeric.py:2348\u001b[0m, in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     dt \u001b[38;5;241m=\u001b[39m multiarray\u001b[38;5;241m.\u001b[39mresult_type(y, \u001b[38;5;241m1.\u001b[39m)\n\u001b[1;32m   2346\u001b[0m     y \u001b[38;5;241m=\u001b[39m asanyarray(y, dtype\u001b[38;5;241m=\u001b[39mdt)\n\u001b[0;32m-> 2348\u001b[0m xfin \u001b[38;5;241m=\u001b[39m \u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2349\u001b[0m yfin \u001b[38;5;241m=\u001b[39m isfinite(y)\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(xfin) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(yfin):\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "num_iterations = 1\n",
    "off_set = 0\n",
    "for i in range(off_set, num_iterations + off_set):\n",
    "    print(f\"Iteration {i}\")\n",
    "    output_df = pd.DataFrame(columns = [\"name\", \"create\", \"update_nodes\", \"update_edges\", \"delete\"])\n",
    "    output_df = eval_ppi(output_df)\n",
    "    output_df = eval_synth(output_df)\n",
    "    output_df.to_csv(f\"neo4j_col_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf37fcc-1f89-40e1-a8f5-d597d441cbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4a65f3706e4757a54efd215837f65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb0f5b8632546818b85385fdf9e7ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ba4a3c5a044973bc544627fef7aae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdbf71ed13e42c78766f688fe31f1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m read_times_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hops \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)):\n\u001b[0;32m---> 31\u001b[0m     read_time, read_time_mem \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_and_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     read_times[hops] \u001b[38;5;241m=\u001b[39m read_time\n\u001b[1;32m     33\u001b[0m     read_times_mem[hops] \u001b[38;5;241m=\u001b[39m read_time_mem\n",
      "Cell \u001b[0;32mIn[6], line 138\u001b[0m, in \u001b[0;36mread\u001b[0;34m(driver, X_and_y, hops, X, y, edge_index, random_sample_size)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m features\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m features_test\n\u001b[0;32m--> 138\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(labels_test, labels), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m labels\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for num_nodes in tqdm([1_000, 10_000, 100_000, 1_000_000]):\n",
    "    for num_edges in tqdm([\"5_edges\", \"10_edges\", \"20_edges\", \"scale_free\"]):\n",
    "        if num_nodes == 1_000_000 and (num_edges == \"10_edges\" or num_edges == \"20_edges\"): continue\n",
    "        feature_file_name = f\"X_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "        label_file_name = f\"y_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "        edge_file_name = f\"edge_index_{str(num_nodes)}_nodes_{num_edges}.csv\"\n",
    "        assert os.path.exists(f\"syn_data/{feature_file_name}\"), \"Feature file does not exist\"\n",
    "        assert os.path.exists(f\"syn_data/{label_file_name}\"), \"Label file does not exist\"\n",
    "        assert os.path.exists(f\"syn_data/{edge_file_name}\"), \"Edge file does not exist\"\n",
    "        \n",
    "        X = pd.read_csv(f\"syn_data/{feature_file_name}\")\n",
    "        X.columns = list(map(lambda col: f\"f_{col}\", X.columns))\n",
    "        y = pd.read_csv(f\"syn_data/{label_file_name}\")\n",
    "        y.columns = list(map(lambda col: f\"y_{col}\", y.columns))\n",
    "        y = y.astype(np.int8)\n",
    "        \n",
    "        edges = pd.read_csv(f\"syn_data/{edge_file_name}\")\n",
    "        edges.columns = [\"source_id\", \"target_id\"]\n",
    "        edge_index = edges.values.transpose(-1, 0)\n",
    "        node_file_name = f\"X_and_y_{str(num_nodes)}_nodes_{num_edges}_neo4j_col.csv\"\n",
    "        if True or not os.path.exists(f\"syn_data/{node_file_name}\"):\n",
    "            X_and_y = X.copy()\n",
    "            X_and_y = pd.concat((X_and_y, y), axis = 1)\n",
    "            X_and_y.to_csv(f\"syn_data/{node_file_name}\", sep = \",\", index = True)\n",
    "        else:\n",
    "            X_and_y = pd.read_csv(f\"syn_data/{node_file_name}\", sep = \";\", index_col = 0)\n",
    "        create_time = create(driver, X_and_y, node_file_name, edge_file_name)\n",
    "        read_times = dict()\n",
    "        read_times_mem = dict()\n",
    "        for hops in tqdm(range(1, 4)):\n",
    "            read_time, read_time_mem = read(driver, X_and_y, hops, X, y, edge_index, random_sample_size= 1000)\n",
    "            read_times[hops] = read_time\n",
    "            read_times_mem[hops] = read_time_mem\n",
    "        update_time_nodes = update_nodes(driver, X_and_y, random_sample_size = 1000)\n",
    "        update_time_edges = update_edges(driver, edge_index, X_and_y, random_sample_size = 1000)\n",
    "        delete_time = delete(driver)\n",
    "        new_row_dict = {\"name\": f\"{str(num_nodes)}_nodes_{num_edges}\", \"create\": create_time, \"update_nodes\": update_time_nodes, \"update_edges\": update_time_edges, \"delete\": delete_time}\n",
    "        for hops in read_times:\n",
    "            new_row_dict[f\"read_{hops}\"] = read_times[hops]\n",
    "            new_row_dict[f\"read_in_mem_{hops}\"] = read_times_mem[hops]\n",
    "        new_row = pd.DataFrame([new_row_dict])\n",
    "        output_df = pd.concat((output_df, new_row), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "309bf4a3-73b8-40d6-842b-c4b52fd6c961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>create</th>\n",
       "      <th>update_nodes</th>\n",
       "      <th>update_edges</th>\n",
       "      <th>delete</th>\n",
       "      <th>read_1</th>\n",
       "      <th>read_in_mem_1</th>\n",
       "      <th>read_2</th>\n",
       "      <th>read_in_mem_2</th>\n",
       "      <th>read_3</th>\n",
       "      <th>read_in_mem_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000_nodes_5_edges</td>\n",
       "      <td>0.724191</td>\n",
       "      <td>14.620965</td>\n",
       "      <td>20.088695</td>\n",
       "      <td>0.094668</td>\n",
       "      <td>23.198426</td>\n",
       "      <td>1.722600</td>\n",
       "      <td>47.961281</td>\n",
       "      <td>2.490153</td>\n",
       "      <td>84.894798</td>\n",
       "      <td>1.766927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000_nodes_10_edges</td>\n",
       "      <td>0.669602</td>\n",
       "      <td>13.031957</td>\n",
       "      <td>19.974057</td>\n",
       "      <td>0.125455</td>\n",
       "      <td>27.695162</td>\n",
       "      <td>1.822084</td>\n",
       "      <td>73.798971</td>\n",
       "      <td>1.832818</td>\n",
       "      <td>217.004385</td>\n",
       "      <td>1.223876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000_nodes_20_edges</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>11.835071</td>\n",
       "      <td>17.506212</td>\n",
       "      <td>0.183648</td>\n",
       "      <td>37.931068</td>\n",
       "      <td>2.024508</td>\n",
       "      <td>119.818688</td>\n",
       "      <td>1.113013</td>\n",
       "      <td>539.471215</td>\n",
       "      <td>2.192494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000_nodes_scale_free</td>\n",
       "      <td>0.330520</td>\n",
       "      <td>13.361930</td>\n",
       "      <td>19.321021</td>\n",
       "      <td>0.081727</td>\n",
       "      <td>4.575462</td>\n",
       "      <td>0.376970</td>\n",
       "      <td>9.635729</td>\n",
       "      <td>0.439245</td>\n",
       "      <td>23.244678</td>\n",
       "      <td>0.415799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000_nodes_5_edges</td>\n",
       "      <td>3.473168</td>\n",
       "      <td>12.913085</td>\n",
       "      <td>66.346031</td>\n",
       "      <td>0.941029</td>\n",
       "      <td>36.675515</td>\n",
       "      <td>2.400790</td>\n",
       "      <td>57.512045</td>\n",
       "      <td>5.750876</td>\n",
       "      <td>98.885532</td>\n",
       "      <td>3.977346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000_nodes_10_edges</td>\n",
       "      <td>4.557181</td>\n",
       "      <td>13.062772</td>\n",
       "      <td>64.708819</td>\n",
       "      <td>1.166279</td>\n",
       "      <td>40.795364</td>\n",
       "      <td>3.255892</td>\n",
       "      <td>86.670575</td>\n",
       "      <td>5.034676</td>\n",
       "      <td>349.527975</td>\n",
       "      <td>4.146428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000_nodes_20_edges</td>\n",
       "      <td>5.495414</td>\n",
       "      <td>13.799639</td>\n",
       "      <td>65.100739</td>\n",
       "      <td>1.766707</td>\n",
       "      <td>49.201019</td>\n",
       "      <td>4.981297</td>\n",
       "      <td>156.750628</td>\n",
       "      <td>4.114111</td>\n",
       "      <td>2083.930239</td>\n",
       "      <td>7.910517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000_nodes_scale_free</td>\n",
       "      <td>3.154809</td>\n",
       "      <td>12.532086</td>\n",
       "      <td>64.797487</td>\n",
       "      <td>0.873238</td>\n",
       "      <td>9.486132</td>\n",
       "      <td>0.465755</td>\n",
       "      <td>28.702118</td>\n",
       "      <td>0.667932</td>\n",
       "      <td>103.862419</td>\n",
       "      <td>0.805704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100000_nodes_5_edges</td>\n",
       "      <td>37.576545</td>\n",
       "      <td>13.858868</td>\n",
       "      <td>560.523144</td>\n",
       "      <td>13.924678</td>\n",
       "      <td>281.266107</td>\n",
       "      <td>10.219358</td>\n",
       "      <td>293.310896</td>\n",
       "      <td>29.860370</td>\n",
       "      <td>331.460629</td>\n",
       "      <td>23.441089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000_nodes_10_edges</td>\n",
       "      <td>50.234440</td>\n",
       "      <td>12.937322</td>\n",
       "      <td>561.987473</td>\n",
       "      <td>16.734241</td>\n",
       "      <td>285.107854</td>\n",
       "      <td>18.198174</td>\n",
       "      <td>317.870894</td>\n",
       "      <td>28.395054</td>\n",
       "      <td>614.013816</td>\n",
       "      <td>26.089577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100000_nodes_20_edges</td>\n",
       "      <td>75.776544</td>\n",
       "      <td>13.261547</td>\n",
       "      <td>560.928216</td>\n",
       "      <td>22.920503</td>\n",
       "      <td>290.757936</td>\n",
       "      <td>30.829016</td>\n",
       "      <td>396.598250</td>\n",
       "      <td>30.251042</td>\n",
       "      <td>3142.308398</td>\n",
       "      <td>50.426474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100000_nodes_scale_free</td>\n",
       "      <td>31.643049</td>\n",
       "      <td>12.451326</td>\n",
       "      <td>560.069617</td>\n",
       "      <td>10.338936</td>\n",
       "      <td>66.427103</td>\n",
       "      <td>1.164215</td>\n",
       "      <td>143.135446</td>\n",
       "      <td>2.509921</td>\n",
       "      <td>1010.903566</td>\n",
       "      <td>3.045121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name     create  update_nodes  update_edges     delete  \\\n",
       "0        1000_nodes_5_edges   0.724191     14.620965     20.088695   0.094668   \n",
       "1       1000_nodes_10_edges   0.669602     13.031957     19.974057   0.125455   \n",
       "2       1000_nodes_20_edges   0.491300     11.835071     17.506212   0.183648   \n",
       "3     1000_nodes_scale_free   0.330520     13.361930     19.321021   0.081727   \n",
       "4       10000_nodes_5_edges   3.473168     12.913085     66.346031   0.941029   \n",
       "5      10000_nodes_10_edges   4.557181     13.062772     64.708819   1.166279   \n",
       "6      10000_nodes_20_edges   5.495414     13.799639     65.100739   1.766707   \n",
       "7    10000_nodes_scale_free   3.154809     12.532086     64.797487   0.873238   \n",
       "8      100000_nodes_5_edges  37.576545     13.858868    560.523144  13.924678   \n",
       "9     100000_nodes_10_edges  50.234440     12.937322    561.987473  16.734241   \n",
       "10    100000_nodes_20_edges  75.776544     13.261547    560.928216  22.920503   \n",
       "11  100000_nodes_scale_free  31.643049     12.451326    560.069617  10.338936   \n",
       "\n",
       "        read_1  read_in_mem_1      read_2  read_in_mem_2       read_3  \\\n",
       "0    23.198426       1.722600   47.961281       2.490153    84.894798   \n",
       "1    27.695162       1.822084   73.798971       1.832818   217.004385   \n",
       "2    37.931068       2.024508  119.818688       1.113013   539.471215   \n",
       "3     4.575462       0.376970    9.635729       0.439245    23.244678   \n",
       "4    36.675515       2.400790   57.512045       5.750876    98.885532   \n",
       "5    40.795364       3.255892   86.670575       5.034676   349.527975   \n",
       "6    49.201019       4.981297  156.750628       4.114111  2083.930239   \n",
       "7     9.486132       0.465755   28.702118       0.667932   103.862419   \n",
       "8   281.266107      10.219358  293.310896      29.860370   331.460629   \n",
       "9   285.107854      18.198174  317.870894      28.395054   614.013816   \n",
       "10  290.757936      30.829016  396.598250      30.251042  3142.308398   \n",
       "11   66.427103       1.164215  143.135446       2.509921  1010.903566   \n",
       "\n",
       "    read_in_mem_3  \n",
       "0        1.766927  \n",
       "1        1.223876  \n",
       "2        2.192494  \n",
       "3        0.415799  \n",
       "4        3.977346  \n",
       "5        4.146428  \n",
       "6        7.910517  \n",
       "7        0.805704  \n",
       "8       23.441089  \n",
       "9       26.089577  \n",
       "10      50.426474  \n",
       "11       3.045121  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6f312-6a9b-451f-8775-728a00bdb832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
