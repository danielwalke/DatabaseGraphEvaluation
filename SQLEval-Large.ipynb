{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6b42e4-fd4e-454e-a694-734a24780d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4780256-9c6c-4236-8819-1a44d676c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO CHeck if we would prefer to set features rather to 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0c39ba-7203-41d8-becd-4a83fab440c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ogb.nodeproppred import NodePropPredDataset\n",
    "\n",
    "# # Download and process data at './dataset/ogbg_molhiv/'\n",
    "# dataset = NodePropPredDataset(name = \"ogbn-papers100M\", root = 'ogbn_dataset/')\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3334040e-f434-4758-bec2-3159212a8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_file_name = \"ogbn_paper100M_edge_index.csv\"\n",
    "# pd.DataFrame(dataset[0][0][\"edge_index\"].transpose(), columns = [\"source_id\", \"target_id\"]).to_csv(f\"data/{edge_file_name}\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb0ce84-88bf-42c2-8a26-6797c5d1926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_file_name = \"ogbn_paper100M_features.csv\"\n",
    "# X_and_y = pd.DataFrame(dataset[0][0][\"node_feat\"]).astype(np.float32)\n",
    "# X_and_y.columns = list(map(lambda col: f\"f_{col}\", X_and_y.columns))\n",
    "# X_and_y[\"label\"] = dataset[0][1].squeeze()\n",
    "# X_and_y[\"label\"] = X_and_y[\"label\"].fillna(-1).astype(np.int16)\n",
    "# X_and_y.to_csv(f\"data/{node_file_name}\", sep = \",\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16eff1d4-df04-43f9-b086-16305ecbf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{node_file_name}\", newline='') as f:\n",
    "  reader = csv.reader(f)\n",
    "  row1 = next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d5b421-443c-4db9-9d3d-a34ed4458f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = row1[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4076de3-f64d-4f72-9731-4efcd5f477b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_execution_time(explain_output):\n",
    "    \"\"\"\n",
    "    Extracts the total execution time from the given query plan.\n",
    "\n",
    "    The function parses the query plan to find the execution time, which is typically\n",
    "    represented in the format 'Execution Time: X ms'. It returns the execution time\n",
    "    in seconds.\n",
    "\n",
    "    Parameters:\n",
    "    query_plan (list): A list of strings representing the lines of the query plan output.\n",
    "\n",
    "    Returns:\n",
    "    float: The total execution time in ms. If the execution time cannot be found,\n",
    "           returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    execution_time = 0.0\n",
    "    pattern = re.compile(r\"Execution Time: (\\d+\\.\\d+) ms\")\n",
    "    for row in explain_output:\n",
    "        match = pattern.search(row[0])\n",
    "        if match:\n",
    "            execution_time += float(match.group(1))\n",
    "    return execution_time\n",
    "    \n",
    "def connect_to_postgres(dbname = \"postgres\"):\n",
    "    \"\"\"Connect to the PostgreSQL database server.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,  # Connect to default db to create new db\n",
    "            user='postgres',\n",
    "            password='password',\n",
    "            host='localhost'\n",
    "        )\n",
    "        print(\"Connection successful.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_database(conn, new_db_name):\n",
    "    \"\"\"Create a new database.\"\"\"\n",
    "    try:\n",
    "        conn.autocommit = True\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql.SQL(\"CREATE DATABASE {};\").format(sql.Identifier(new_db_name)))\n",
    "            print(f\"Database '{new_db_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.autocommit = False\n",
    "\n",
    "def create_schema(conn, schema_sql):\n",
    "    \"\"\"Create the database schema.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(schema_sql)\n",
    "            conn.commit()\n",
    "            print(\"Schema created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating schema: {e}\")\n",
    "        raise\n",
    "\n",
    "def upload_csv_to_table(conn, csv_file_path, table_name):\n",
    "    \"\"\"Upload a CSV file to a table using the COPY command.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            with open(csv_file_path, 'r') as f:\n",
    "                cursor.copy_expert(\n",
    "                    sql.SQL(\"\"\"\n",
    "                        COPY {} FROM STDIN WITH (FORMAT CSV, HEADER TRUE, DELIMITER ',');\n",
    "                    \"\"\").format(sql.Identifier(table_name)), f\n",
    "                )\n",
    "            conn.commit()\n",
    "            print(f\"Data from '{csv_file_path}' uploaded to table '{table_name}' successfully using COPY.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error uploading CSV data using COPY: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_index(conn, table_name, column_name):\n",
    "    \"\"\"Create an index on a specific column.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            index_name = f\"{table_name}_{column_name}_idx\"\n",
    "            cursor.execute(sql.SQL(\"CREATE INDEX {} ON {} ({});\").format(\n",
    "                sql.Identifier(index_name),\n",
    "                sql.Identifier(table_name),\n",
    "                sql.Identifier(column_name)\n",
    "            ))\n",
    "            conn.commit()\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating index: {e}\")\n",
    "        raise\n",
    "\n",
    "def read_data(conn, query):\n",
    "    \"\"\"Read data from the database.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            print(\"Data read successfully.\")\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def update_data(conn, query, params):\n",
    "    \"\"\"Update data in the database.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, params)\n",
    "            conn.commit()\n",
    "            print(\"Data updated successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error updating data: {e}\")\n",
    "        raise\n",
    "\n",
    "def delete_database(conn, db_name):\n",
    "    \"\"\"Delete the specified database including all its schemas, tables, and indexes.\"\"\"\n",
    "    try:\n",
    "        conn.autocommit = True\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql.SQL(\"DROP DATABASE IF EXISTS {};\").format(sql.Identifier(db_name)))\n",
    "            print(f\"Database '{db_name}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.autocommit = False\n",
    "\n",
    "def create_edges_table(conn):\n",
    "    \"\"\"Create an edges table with foreign key constraints to the main table.\"\"\"\n",
    "    edges_schema = \"\"\"\n",
    "    CREATE TABLE edges (\n",
    "        source_id INTEGER NOT NULL,\n",
    "        target_id INTEGER NOT NULL,\n",
    "        FOREIGN KEY (source_id) REFERENCES nodes(id) ON DELETE CASCADE,\n",
    "        FOREIGN KEY (target_id) REFERENCES nodes(id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(edges_schema)\n",
    "            conn.commit()\n",
    "            print(\"Edges table schema created successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error creating edges table: {e}\")\n",
    "        raise\n",
    "\n",
    "def upload_edges_csv_to_table(conn, csv_file_path):\n",
    "    \"\"\"Upload a CSV file to the edges table using the COPY command.\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            with open(csv_file_path, 'r') as f:\n",
    "                cursor.copy_expert(\n",
    "                    sql.SQL(\"\"\"\n",
    "                        COPY edges (source_id, target_id) FROM STDIN WITH (FORMAT CSV, HEADER TRUE, DELIMITER ',');\n",
    "                    \"\"\"), f\n",
    "                )\n",
    "            conn.commit()\n",
    "            print(f\"Edges data from '{csv_file_path}' uploaded successfully.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error uploading edges CSV data: {e}\")\n",
    "        raise\n",
    "\n",
    "def recurse_edge_index_iterative(source_nodes, edge_index, max_depth):\n",
    "    \"\"\"\n",
    "    Optimized function to compute the subgraph around the source nodes up to a given depth.\n",
    "    Uses an iterative approach instead of recursion.\n",
    "    \"\"\"\n",
    "    visited_nodes = set(source_nodes)\n",
    "    current_frontier = np.array(source_nodes)\n",
    "    \n",
    "    subgraph_edges = []\n",
    "\n",
    "    for _ in range(max_depth):\n",
    "        # Find edges where the target node is in the current frontier\n",
    "        target_mask = np.isin(edge_index[1], current_frontier)\n",
    "        subgraph_edge_index = edge_index[:, target_mask]\n",
    "        subgraph_edges.append(subgraph_edge_index)\n",
    "\n",
    "        # Update the current frontier with the source nodes of these edges\n",
    "        current_frontier = np.setdiff1d(subgraph_edge_index[0], list(visited_nodes))\n",
    "        visited_nodes.update(current_frontier)\n",
    "        \n",
    "        if len(current_frontier) == 0:\n",
    "            break\n",
    "\n",
    "    # Combine edges from all hops\n",
    "    return np.concatenate(subgraph_edges, axis=1) if subgraph_edges else np.empty((2, 0), dtype=edge_index.dtype)\n",
    "\n",
    "\n",
    "def get_subgraph_from_in_mem_graph_optimized(X, y, i, edge_index, hops):\n",
    "    \"\"\"\n",
    "    Optimized version of subgraph extraction.\n",
    "    \"\"\"\n",
    "    subgraph_edge_index = recurse_edge_index_iterative([i], edge_index, hops)\n",
    "    unique_node_ids, remapping = np.unique(subgraph_edge_index, return_inverse=True)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = X.iloc[unique_node_ids, :].values\n",
    "    labels = y.iloc[unique_node_ids, :].values.squeeze()\n",
    "\n",
    "    # Remap edge indices\n",
    "    remapped_edge_index = remapping.reshape(2, -1)\n",
    "    return remapped_edge_index, features, labels, unique_node_ids\n",
    "\n",
    "def create_db(node_file_name):\n",
    "    # Create a new database\n",
    "    conn = connect_to_postgres(dbname = \"postgres\")\n",
    "    new_db_name = node_file_name.split(\".\")[0]\n",
    "    create_database(conn, new_db_name)\n",
    "    conn.close()\n",
    "    conn = connect_to_postgres(new_db_name)\n",
    "    return conn, new_db_name\n",
    "\n",
    "def create(conn, node_file_name, edge_file_name, columns = columns):\n",
    "    # column_types = [\"id SERIAL PRIMARY KEY\"]\n",
    "    # for col in X_and_y.columns:\n",
    "    #     if col == \"label\":\n",
    "    #         column_types.append(f\"{col} INTEGER\")\n",
    "    #         continue\n",
    "    #     column_types.append(f\"{col} REAL\")\n",
    "        \n",
    "    # node_schema = f\"\"\"\n",
    "    # CREATE TABLE nodes (\n",
    "    #     {\",\".join(column_types)}\n",
    "    # );\n",
    "    # \"\"\"\n",
    "    start = time.time()\n",
    "    # create_schema(conn, node_schema)\n",
    "    # create_edges_table(conn)\n",
    "    \n",
    "    # csv_file_path = f\"data/{node_file_name}\"  # Replace with your CSV file path\n",
    "    # upload_csv_to_table(conn, csv_file_path, \"nodes\")\n",
    "    # create_index(conn, \"nodes\", \"id\")\n",
    "    upload_edges_csv_to_table(conn, f\"data/{edge_file_name}\")\n",
    "    create_index(conn, \"edges\", \"target_id\")\n",
    "    creation_time = time.time() - start\n",
    "    return creation_time\n",
    "\n",
    "def read(conn, hops, columns = columns):\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT id FROM nodes\")\n",
    "        results = cursor.fetchall()\n",
    "    seed_node_ids = list(map(lambda res_data: res_data[0], results))\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        complete_time = 0\n",
    "        complete_test_time = 0\n",
    "        for seed_node_id in tqdm(seed_node_ids):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                cursor.execute(f\"\"\"\n",
    "        WITH RECURSIVE NestedTargets AS (\n",
    "            SELECT 0 AS depth, source_id, target_id\n",
    "            FROM edges\n",
    "            WHERE target_id = {seed_node_id}\n",
    "            \n",
    "            UNION ALL\n",
    "            \n",
    "            SELECT nt.depth + 1, e.source_id, e.target_id\n",
    "            FROM edges e\n",
    "            JOIN NestedTargets nt ON e.target_id = nt.source_id\n",
    "            WHERE nt.depth < {hops - 1}\n",
    "        ),\n",
    "        \n",
    "        node_ids AS (\n",
    "            SELECT DISTINCT id FROM (\n",
    "                SELECT source_id AS id FROM NestedTargets\n",
    "                UNION\n",
    "                SELECT target_id AS id FROM NestedTargets\n",
    "            ) AS combined_ids\n",
    "        ),\n",
    "        \n",
    "        node_data AS (\n",
    "            SELECT \n",
    "                id, \n",
    "                {\", \".join(columns)}\n",
    "            FROM nodes\n",
    "            WHERE id IN (SELECT id FROM node_ids)\n",
    "            ORDER BY id\n",
    "        )\n",
    "        \n",
    "        SELECT\n",
    "            (SELECT array_agg(array[id, {\", \".join(columns[:-1])}]) FROM node_data) AS node_table,\n",
    "            (SELECT array_agg({columns[-1]}) FROM node_data) AS label_table,\n",
    "            (SELECT array_agg(array[source_id, target_id])\n",
    "             FROM (SELECT DISTINCT source_id, target_id FROM NestedTargets) AS edges) AS edge_table,\n",
    "             (SELECT array_agg(id) FROM node_data) AS node_ids; \n",
    "    \"\"\") ##Cant use id from above since array agg is float convertng my ints which reduces the max value for the serial integer (e.g., 10,000,323 -> 10,000,320)\n",
    "                results = cursor.fetchall()[0]\n",
    "                labels = np.array(results[1])\n",
    "                subgraph_node_features = np.array(results[0])\n",
    "                if results[0] is None:\n",
    "                    continue\n",
    "                \n",
    "                subgraph_edges = np.array(results[2]).transpose()\n",
    "                node_ids = np.array(results[-1])\n",
    "                _, cols_source = np.nonzero((subgraph_edges[0] == node_ids[:, None]).transpose())\n",
    "                _, cols_target = np.nonzero((subgraph_edges[1] == node_ids[:, None]).transpose())\n",
    "\n",
    "                remapped_edge_index = np.concatenate([np.expand_dims(cols_source, axis = 0), np.expand_dims(cols_target, axis = 0)], axis = 0)\n",
    "                features = subgraph_node_features[:, 1:]\n",
    "                overall_run_time = time.time() - start \n",
    "                \n",
    "                complete_time += overall_run_time\n",
    "                \n",
    "                # Testing\n",
    "                test_time = time.time()\n",
    "                # remapped_edge_index_test, features_test, labels_test, unique_node_ids = get_subgraph_from_in_mem_graph_optimized(X, y, seed_node_id, edge_index, hops)                    \n",
    "                # complete_test_time += time.time() - test_time\n",
    "                # assert (sort_edge_index(torch.from_numpy(remapped_edge_index_test)) == sort_edge_index(torch.from_numpy(remapped_edge_index))).sum() / (remapped_edge_index_test.shape[-1] * remapped_edge_index_test.shape[0]), \"Edges doesnt match\"\n",
    "                # assert np.allclose(features, features_test), \"features doe not match\"\n",
    "                # assert np.allclose(labels_test, labels), \"Labels does not match\"\n",
    "                # print(f\"Fetched {remapped_edge_index.shape} edges, {labels.shape} labels, {features.shape} features in ({overall_run_time} s)\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                print(f\"Error reading subgraphs: {e}\")\n",
    "                raise   \n",
    "    return (complete_time, complete_test_time)\n",
    "\n",
    "def delete(conn, new_db_name):\n",
    "    start = time.time()\n",
    "    conn.close()\n",
    "    conn = connect_to_postgres()\n",
    "    delete_database(conn, new_db_name)\n",
    "    conn.close()\n",
    "    return time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7eb25b40-6eca-4a48-9f5c-2dec8a8a03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "hops = 2\n",
    "overall_run_time = 0\n",
    "# edge_index = dataset[0][0][\"edge_index\"] # edges.values.astype(np.int64).transpose()\n",
    "\n",
    "# for i in tqdm(range(X.shape[0])):\n",
    "#     start = time.time()\n",
    "#     remapped_edge_index, features, labels, node_ids = get_subgraph_from_in_mem_graph_optimized(X, y, i, edge_index, hops)\n",
    "#     overall_run_time += time.time() - start\n",
    "    \n",
    "#     print(f\"Fetched {remapped_edge_index.shape} edges, {labels.shape} labels, {features.shape} features in {overall_run_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8edf2bae-7abb-4861-9445-94a2d574d226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conn, new_db_name = create_db(node_file_name)\n",
    "# create_time = create(conn, node_file_name, edge_file_name, X_and_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c63422-e400-4d77-bc0f-dedcb4fe6051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfea3a519e1b4e49aa37e6f6bbbc699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111059956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m read_time, read_time_mem \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 261\u001b[0m, in \u001b[0;36mread\u001b[0;34m(conn, hops, columns)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m             start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 261\u001b[0m             \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;43m    WITH RECURSIVE NestedTargets AS (\u001b[39;49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;43m        SELECT 0 AS depth, source_id, target_id\u001b[39;49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;43m        FROM edges\u001b[39;49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;43m        WHERE target_id = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed_node_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;43m        UNION ALL\u001b[39;49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;43m        SELECT nt.depth + 1, e.source_id, e.target_id\u001b[39;49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;43m        FROM edges e\u001b[39;49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;43m        JOIN NestedTargets nt ON e.target_id = nt.source_id\u001b[39;49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;43m        WHERE nt.depth < \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhops\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;43m    ),\u001b[39;49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;43m    node_ids AS (\u001b[39;49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;43m        SELECT DISTINCT id FROM (\u001b[39;49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;43m            SELECT source_id AS id FROM NestedTargets\u001b[39;49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;43m            UNION\u001b[39;49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;43m            SELECT target_id AS id FROM NestedTargets\u001b[39;49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;43m        ) AS combined_ids\u001b[39;49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;43m    ),\u001b[39;49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;43m    node_data AS (\u001b[39;49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;43m        SELECT \u001b[39;49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;43m            id, \u001b[39;49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;43m        FROM nodes\u001b[39;49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;43m        WHERE id IN (SELECT id FROM node_ids)\u001b[39;49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;43m        ORDER BY id\u001b[39;49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;43m    )\u001b[39;49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;43m    SELECT\u001b[39;49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;43m        (SELECT array_agg(array[id, \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]) FROM node_data) AS node_table,\u001b[39;49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;43m        (SELECT array_agg(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) FROM node_data) AS label_table,\u001b[39;49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;43m        (SELECT array_agg(array[source_id, target_id])\u001b[39;49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;43m         FROM (SELECT DISTINCT source_id, target_id FROM NestedTargets) AS edges) AS edge_table,\u001b[39;49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;43m         (SELECT array_agg(id) FROM node_data) AS node_ids; \u001b[39;49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m##Cant use id from above since array agg is float convertng my ints which reduces \u001b[39;00m\n\u001b[1;32m    299\u001b[0m             results \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchall()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    300\u001b[0m             labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.10/encodings/utf_8.py:15\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m### Codec APIs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m encode \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mutf_8_encode\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28minput\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mutf_8_decode(\u001b[38;5;28minput\u001b[39m, errors, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalEncoder\u001b[39;00m(codecs\u001b[38;5;241m.\u001b[39mIncrementalEncoder):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "read_time, read_time_mem = read(conn, 3, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb3dfe-ff9d-4277-84ef-8a1a6f27b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_time, read_time_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a97157-d536-4953-962b-2e44414445cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_times = dict()\n",
    "read_times_mem = dict()\n",
    "for hops in tqdm(range(1, 4)):\n",
    "    read_time, read_time_mem = read(conn, hops)\n",
    "    read_times[hops] = read_time\n",
    "    read_times_mem[hops] = read_time_mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3319dc2f-369a-4968-9a39-e92f109071da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n",
      "Connection successful.\n",
      "Error deleting database: database \"ogbn_paper100M_features\" is being accessed by other users\n",
      "DETAIL:  There is 1 other session using the database.\n",
      "\n"
     ]
    },
    {
     "ename": "ObjectInUse",
     "evalue": "database \"ogbn_paper100M_features\" is being accessed by other users\nDETAIL:  There is 1 other session using the database.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mObjectInUse\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m new_db_name \u001b[38;5;241m=\u001b[39m node_file_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m conn \u001b[38;5;241m=\u001b[39m connect_to_postgres(new_db_name)\n\u001b[0;32m----> 3\u001b[0m delete_time \u001b[38;5;241m=\u001b[39m \u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_db_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 333\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(conn, new_db_name)\u001b[0m\n\u001b[1;32m    331\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    332\u001b[0m conn \u001b[38;5;241m=\u001b[39m connect_to_postgres()\n\u001b[0;32m--> 333\u001b[0m \u001b[43mdelete_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_db_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[0;32mIn[11], line 128\u001b[0m, in \u001b[0;36mdelete_database\u001b[0;34m(conn, db_name)\u001b[0m\n\u001b[1;32m    126\u001b[0m     conn\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[0;32m--> 128\u001b[0m         \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSQL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP DATABASE IF EXISTS \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdentifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m deleted successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mObjectInUse\u001b[0m: database \"ogbn_paper100M_features\" is being accessed by other users\nDETAIL:  There is 1 other session using the database.\n"
     ]
    }
   ],
   "source": [
    "delete_time = delete(conn, new_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57f56212-ee01-4f05-97cb-a4315fa9cb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 270.018767118454, 2: 3174.6598134040833, 3: 49607.77852463722}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cace5505-72a2-4ced-a3a3-c92efdfd4467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 331.7824068069458, 2: 604.4689948558807, 3: 1079.6200346946716}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_times_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1c03f1c-5562-4bcf-ba9e-5338add89bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.816965341567993"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e4e84c6-2447-4a5f-a4e5-371301edf708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10642385482788086"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655aa6b6-7dfc-4f7a-97d6-1f7f913ab165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "new_db_name = node_file_name.split(\".\")[0]\n",
    "conn = connect_to_postgres(new_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73cc797e-f46e-42dc-a26a-3e012a7dfc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1615685872,)]\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT count(*) FROM edges \")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d160e-d367-409e-b49c-3949db0336ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Check if everythign runs without problems\n",
    "## TODO Update function\n",
    "## TODO Neo4j impl\n",
    "## TODO MySQL impl\n",
    "## TODO large graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b909a3-3238-481d-87c5-8204871717de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update data\n",
    "# update_query = \"UPDATE example_table SET city = %s WHERE id = %s;\"\n",
    "# update_params = (\"NewCity\", 1)\n",
    "# update_data(conn, update_query, update_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
